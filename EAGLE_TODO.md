# TurboMind EAGLE Integration TODO

This file tracks all work needed to bring TurboMind‚Äôs EAGLE speculative decoding to full, production use. Items marked `[x]` are already implemented in this repo; items marked `[ ]` are pending.

Status Legend:
- ‚úÖ Implemented + tested (production-ready for current scope)
- üß™ Implemented (prototype; GPU/CI validation pending or limited in scope)
- ‚è≥ Planned (not implemented)

## 1. Engine config and initialization

- [x] Parse `speculative_config` into `TurbomindEngineConfig` and `EngineParam` (including `method`, `model`, `num_speculative_tokens`, `max_path_len`, `max_decoding_tokens`, `max_non_leaves_per_layer`).
- [x] Initialize EAGLE for Llama models:
  - [x] Add speculative fields to `EngineParam` / `llama_params.h`.
  - [x] In `LlamaV2` ctor, when `enable_speculative_decoding` and `spec_method in {"eagle", "eagle3"}`:
    - [x] Set `spec_mode_ = SpeculativeDecodingMode::Eagle()`.
    - [x] Construct `EagleModule` with limits from `EngineParam`.
    - [x] Load draft model weights from `spec_draft_model_path`.
    - [x] Construct `EagleBuffers`.
- [x] Add logging so we can see when/with what config EAGLE is initialized (`[LlamaTritonModel]`, `[LlamaV2]`).

## 2. Core EAGLE data structures and kernels

- [x] Implement TurboMind‚Äëside EAGLE tree representation:
  - [x] `lmdeploy/turbomind/eagle_tree.h` / `.cpp` for building and flattening speculation trees.
  - [x] Replace the initial linear `SpeculationTree::buildTree` / `EagleModule::initializeDefaultChoices` placeholder with a config‚Äëdriven EAGLE3 branching tree structure (`eagle_tree.yaml` choices with a simple chain as a safe fallback).
- [x] Implement CUDA kernels for EAGLE masks:
  - [x] `invokeBuildLeafMask` ‚Äì mark leaf/non‚Äëleaf nodes.
  - [x] `invokeGetPackedMaskFromPath` ‚Äì build packed attention masks from tree paths.
- [x] Implement `EagleBuffers` for TurboMind:
  - [x] GPU buffers for draft tokens, paths, packed masks, and acceptance metadata.
  - [x] Clean allocation/free (no leaked `Tensor` objects, raw CUDA pointers only).
- [x] Implement `LlamaV2::eagleSpeculativeStep` to:
  - [x] Build the speculation tree from `draft_tokens`.
  - [x] Copy paths to device buffers.
  - [x] Call EAGLE CUDA kernels to produce leaf masks and packed masks.
  - [x] Populate acceptance rate metrics based on real comparisons between draft and target tokens (one-step, per-sequence acceptance for the initial integration).

## 3. Draft model (EagleNet) support

- [x] Implement `EagleModule::load`:
  - [x] Parse draft model config (hidden size, vocab size, intermediate size, etc.).
  - [x] Allocate FP16 `Tensor` weights for all required parameters (embeddings, FC, norms, attention, MLP, LM head, mapping).
  - [x] Load binary weight files into device tensors with typed copies.
- [x] Implement `EagleModule::forward`:
  - [x] Implement a minimal, production‚Äësafe draft network using existing kernels (RMSNorm + LM head over target hidden states) to produce real draft logits.
  - [x] Extend the draft network to use more of the EagleNet stack (at least one shallow attention+FC ‚ÄúMLP‚Äù block) with the loaded weights, reusing `LlamaLinear` and pre‚Äëformatted `LlamaDenseWeight` wrappers instead of custom GEMMs.
  - [x] Ensure the forward path is free of `cudaMalloc`/`cudaFree` in hot loops by routing all temporary activations via internal scratch tensors (and, later, `EagleBuffers`) that are sized once and reused.

## 4. Speculative decode loop integration in LlamaBatch / LlamaV2

Status: Phase‚Äë1 EAGLE branch and engine‚Äëtoken logging are implemented; the multi‚Äëtoken inner loop and strict enforcement of
`eagle_max_engine_tokens_per_step_` as a hard budget (plus sequence advancement) remain TODO.

- [ ] Introduce EAGLE‚Äëaware ‚Äúengine tokens per step‚Äù semantics:
  - [x] Add a `eagle_max_engine_tokens_per_step_` field in `LlamaV2` and initialize it from `spec_max_decoding_tokens` clamped by `max_forward_token_num`.
  - [x] In `LlamaBatch::Forward`, plumb this budget into the EAGLE logs so each mini‚Äëbatch reports the configured engine token budget alongside actual `engine_tokens`.
  - [ ] Use this budget to drive the speculative inner loop once multi‚Äëtoken EAGLE decoding is wired, keeping per‚Äësequence engine shapes static where possible so we can later enable CUDA‚Äëgraph capture for offline decode steps. For now, `compute_eagle_draft_tokens_per_seq` computes a planned `tokens_per_seq` from this budget but clamps the effective value to `1` so that no partially‚Äëinitialized multi‚Äëtoken layouts are exposed before the full inner loop is implemented.
- [x] Add a speculative branch in `LlamaBatch::Forward`:
  - [x] For each decode step (or mini‚Äëbatch), add a guarded `[LlamaBatch][EAGLE]` branch that:
    - [x] Captures the last‚Äëtoken hidden states from `LlamaV2`‚Äôs decoder output buffer.
    - [x] Invokes `LlamaV2::eagleDraftForward` (a thin wrapper around `EagleModule::forward`) to generate per‚Äësequence draft logits.
    - [x] Samples one draft token per active sequence (greedy argmax for now) and populates a host `draft_tokens` buffer.
    - [x] Calls `LlamaV2::eagleSpeculativeStep` with these `draft_tokens` and the current `Sequence*` array, logging per‚Äëstep EAGLE activity.
  - [ ] Use `num_accepted` / `accepted_tokens` to decide how far to advance each sequence in this step (for the one-step case, we only verify consistency and log metrics; multi-token advancement will be added later).

## 5. Target verification and acceptance (LlamaV2_eagle)

Status: One‚Äëstep host‚Äëside target verification and tree‚Äëbased host acceptance are implemented and mirrored into `EagleBuffers`; the
generalised, fully device‚Äëside tree verification for multi‚Äëtoken steps remains TODO.

- [ ] Implement full ‚Äútarget verify‚Äù logic in `LlamaV2_eagle.cc`:
  - [ ] Extend `EagleBuffers::inputs` / `outputs` as needed to hold:
    - [ ] Per‚Äënode logits indices / hidden‚Äëstate references.
    - [ ] Accepted paths and lengths per sequence.
  - [x] Add a helper path that, for the initial integration, uses `LlamaBatch`‚Äëprovided greedy target token IDs to implement a correct one‚Äëstep acceptance rule:
    - [x] Extend `LlamaV2::eagleSpeculativeStep` to accept both `draft_tokens` and `target_tokens`.
    - [x] For each active sequence, compare `target_token == draft_token` and:
      - [x] Write the accepted draft token into the host `accepted_tokens` buffer when they match.
      - [x] Accumulate a scalar `num_accepted` and log `[EAGLE] Accepted N/M` based on real target tokens.
    - [x] Mirror these host‚Äëside accepted tokens/lengths into `EagleBuffers::outputs.accepted_tokens` / `accepted_lens` on device so later stages (KV/cache updates, DynamicDecode integration) can consume them.
  - [ ] Generalize this to full tree‚Äëbased verification:
    - [ ] Prepare embeddings and masks for the tree nodes using the packed masks.
    - [ ] Call `LlamaV2::Forward` (or a dedicated decoder path) once per step to compute target logits for all draft nodes in the batch (no per‚Äënode launches).
    - [ ] For each sequence, compare target logits‚Äô argmax vs draft token ids along each path using GPU‚Äëside kernels only.
  - [x] Implement acceptance rule for full trees (host‚Äëside for now):
    - [x] For each sequence, walk every path in the `SpeculationTree` and count how many tokens would be appended under the ‚Äúaccept while draft==target, append target on first mismatch‚Äù rule.
    - [x] Select the path with the longest accepted length per sequence and materialize `accepted_tokens` / `accepted_lens` on the host.
    - [x] Mirror host‚Äëside `accepted_tokens` / `accepted_lens` and `best_path_ids` into `EagleBuffers::outputs` / `inputs` and call `invokePackAcceptedPaths` so downstream KV/decode code sees a packed, tree‚Äëconsistent view of accepted paths. A future follow‚Äëup can move this acceptance computation into a dedicated GPU kernel once multi‚Äëtoken target logits are available.

## 6. KV cache and decode‚Äëstate integration

Status: KV over‚Äëprovisioning and the KV rewind helper/kernel are implemented and tested; wiring accepted tokens into
`token_ids_buf_` / `sequence_lengths_` and integrating rewind with `SequenceManager`/prefix caching are still TODO.

- [ ] Update TurboMind KV cache based on EAGLE acceptance:
  - [ ] In `LlamaBatch::Forward`, after `eagleSpeculativeStep`, adjust:
    - [ ] `token_ids_buf_` (so future steps see accepted tokens as committed).
    - [ ] `sequence_lengths_` and any other per‚Äësequence length/state fields.
    - [ ] KV‚Äëcache lengths and block indices for rejected tokens (rewind).
    - [x] Implement `invokeKVCacheRewind` in `lmdeploy/turbomind/kernels/speculative_decoding/common.{h,cu}` so that it marks tail KV blocks as logically free in a per‚Äësequence block table (block IDs and optional `kv_cache_blocks` pointers), ready to be driven by TurboMind‚Äôs real KV cache / block manager.
  - [ ] Ensure compatibility with:
    - [ ] Prefix caching.
    - [ ] Model parallelism (TP/DP/CP) and cache layout.
- [ ] Verify that non‚Äëspeculative decoding (no EAGLE) remains bit‚Äëidentical to current behaviour.
 - [x] For EAGLE‚Äëenabled models, over‚Äëprovision KV cache when initializing `SequenceManager` so spec decoding has room for both tree nodes and accepted tokens:
   - [x] Compute an ‚ÄúEAGLE KV factor‚Äù from `spec_max_decoding_tokens` and `spec_max_draft_path_len` and fold it into the effective `cache_max_block_count` passed to `SequenceManager`.
   - [x] Add logs to make it easy to see when EAGLE KV over‚Äëprovisioning is active and what effective KV block budget is used for offline batches.

## 7. Sampling / DynamicDecodeLayer integration

Status: Multi‚Äëtoken integration with `DynamicDecodeLayer` is not yet implemented; EAGLE currently uses baseline
`dynamicDecode` semantics with a metrics‚Äëonly acceptance path.

- [ ] Decide integration strategy:
  - [ ] Option A: Extend `DynamicDecodeLayer` to understand EAGLE acceptance and variable numbers of tokens per step.
  - [ ] Option B: When `spec_mode_.isEagle()`, manage sampling and token advancement entirely inside `LlamaBatch::Forward`, bypassing `DynamicDecodeLayer` for speculative steps.
- [ ] Implement chosen strategy:
  - [ ] Ensure that:
    - [ ] `output_ids`, `finished`, and `sequence_length` reflect accepted draft tokens.
    - [ ] Logit sampling, stop criteria, and logprobs behave correctly with multi‚Äëtoken steps.

## 8. Metrics, logging, and validation

Status: Logging hooks, C++ EAGLE metrics, Python `spec_info` plumbing, and most unit/integration tests (including benchmark
integration) are in place; per‚Äëbackend throughput metrics are currently surfaced via scripts, and dedicated multi‚Äëtoken
validation tests are still TODO.

- [x] Add logging hooks:
  - [x] `[LlamaBatch][EAGLE]` per step/mini‚Äëbatch with engine token / context token counts.
  - [x] `[EAGLE]` logs from `LlamaV2::eagleSpeculativeStep` for tree/mask construction.
- [ ] Add runtime metrics:
  - [x] Per‚Äërequest acceptance rate and average accepted tokens per step exported from TurboMind‚Äôs C++ backend (not just Python‚Äëside wrappers).
  - [x] Effective throughput vs baseline (tokens/s, ms/token) for EAGLE and non‚ÄëEAGLE TurboMind runs via `inference/benchmark_speculative.py`, with JSON output validated by `lmdeploy/tests/test_benchmark_speculative_integration.py`.
  - [ ] Extend throughput reporting to the PyTorch backend and ensure both backends expose comparable EAGLE vs baseline metrics.
- [ ] Validation:
  - [x] Unit tests for `EagleModule::forward` on small synthetic examples (via `_turbomind.eagle_forward_smoke` and `test_eagle_module.py`).
  - [x] Unit tests for `eagle_tree` on small synthetic examples.
  - [x] Integration test skeleton for EAGLE vs baseline behaviour in `lmdeploy/tests/test_eagle_e2e.py`, gated on `MODEL_PATH` / `SPEC_MODEL_PATH` so it can be enabled in environments with accessible models.
  - [x] Standalone benchmark harness (`inference/benchmark_speculative.py`) wired to compare baseline vs EAGLE TurboMind across multiple scenarios, including short single-sequence runs suitable for local validation.

## 9. Hard constraints / non‚Äëgoals for this port

- The implementation must:
  - Avoid copying TensorRT‚ÄëLLM source code verbatim.
  - Preserve existing TurboMind decoding behaviour when EAGLE is disabled.
  - Fail safely (e.g., treat `num_accepted = 0`) if the draft model or EAGLE path is misconfigured, rather than returning incorrect tokens.

## 10. Legacy Python speculative wrappers (cleanup)

- [x] Align `SpeculativeGenerationWrapper` with native TurboMind EAGLE:
  - [x] Remove the ‚Äúaccept first 2 draft tokens‚Äù stub path (used only in tests) or gate it behind a dedicated test‚Äëonly flag; rely on `DraftTokenVerifier` + real/synthetic target logits for acceptance metrics (no unconditional acceptance in production paths).
- [x] Clarify / scope `SpeculativeDecodingManager` and batch wrappers:
  - [x] For `method in {"eagle", "eagle3"}`, rely solely on native C++ EAGLE (no Python‚Äëside draft model); keep these managers for `draft_target` / `ngram` and tests, and add tests to enforce this behaviour.
  - [x] Implement buffer reuse logic in `OptimizedBatchSpeculativeManager` (pre‚Äëallocate `draft_buffer` / `packed_mask_buffer` and fill `draft_buffer` for the latest batch) instead of keeping it as a stub, so production batch workloads avoid per‚Äëstep allocations.

## 11. Engineer‚Äëlevel work split (coordination)

To avoid overlap between engineers working on TurboMind EAGLE, we track who owns which slices of the above TODOs.

### Engineer A (this plan) ‚Äì EagleModule / KV / metrics / Python wrappers

The tasks below are the 10 concrete items Engineer A is responsible for, all drawn from the TODOs above and chosen to avoid LlamaBatch / `LlamaV2_eagle` / sampling work owned by Engineer B. Items 1‚Äì10 are now implemented; the next focus areas (Phase 2) are additional optimizations, metrics, and safety hardening in EagleModule/KV/Python wrappers that other engineers can rely on.

1. **Maintain config‚Äëdriven EAGLE tree choices and defaults**  
   - From section 2: keep `SpeculationTree` / `EagleModule::getDefaultChoices` wired to `eagle_tree.yaml` `choices` for offline‚Äëtuned trees, and ensure the fallback simple chain remains a safe, well‚Äëdocumented default (no demo‚Äëonly behaviour).

2. **Run a shallow EagleNet attention+FC block inside `EagleModule::forward`**  
   - From section 3: use a single self‚Äëattention (QKV + Wo) plus FC ‚ÄúMLP‚Äù block, implemented via `LlamaLinear` with pre‚Äëformatted `LlamaDenseWeight` wrappers, between input/output RMSNorms so the draft network is deeper than RMSNorm+LM head while staying lightweight.

3. **Keep `EagleModule::forward` allocation‚Äëfree in the hot path**  
   - From section 3: route pre‚Äëattn inputs, QKV, attention outputs, FC inputs/outputs and final normed activations through reusable scratch tensors so no per‚Äëstep `cudaMalloc`/`cudaFree` occurs during speculative decode.

4. **Provide a usable `invokeKVCacheRewind` kernel for per‚Äësequence block tables**  
   - From section 6: implement `invokeKVCacheRewind` in `lmdeploy/turbomind/kernels/speculative_decoding/common.{h,cu}` so that it marks tail KV blocks as logically free in a `[batch, max_blocks_per_seq]` block table (and nulls corresponding `kv_cache_blocks` entries), ready to be wired into TurboMind‚Äôs real KV cache manager.

5. **Over‚Äëprovision KV cache for EAGLE‚Äëenabled TurboMind engines**  
   - From section 6: fold an ‚ÄúEAGLE KV factor‚Äù derived from `spec_max_decoding_tokens` and `spec_max_draft_path_len` into the effective `cache_max_block_count` when instantiating `SequenceManager` in `LlamaBatch::InitializeBufferAndKVCache`, with clear logging on rank‚Äë0.

6. **Export and plumb TurboMind EAGLE acceptance metrics end‚Äëto‚Äëend**  
   - From section 8: extend C++ `RequestMetrics` with EAGLE totals, expose them via the `_turbomind.RequestMetrics` binding, and populate `RequestMetrics.spec_info` in `_get_metrics` so `SpeculativeDecodingStats` in `lmdeploy/metrics/stats.py` can consume TurboMind EAGLE stats just like the PyTorch engine.

7. **Add focused unit tests for `eagle_tree`**  
   - From section 8: test config‚Äëdriven tree construction (choices table, `max_depth` / `max_paths` clipping) and path flattening, including edge cases like empty choices and degenerate chains.

8. **Add focused unit tests for `EagleModule::forward`**  
   - From section 8: with a tiny synthetic config/weights, validate shapes and basic numerical sanity of the shallow attention+FC+LM head pipeline, and ensure that forwarding twice with the same batch size reuses scratch buffers instead of reallocating (implemented via `_turbomind.eagle_forward_smoke` and `test_eagle_module.py`).

9. **Design host‚Äëside EAGLE KV rewind helper callable from LlamaBatch**  
   - From section 6: provide a helper (outside `LlamaBatch` / `LlamaV2_eagle`) that turns per‚Äësequence draft / accepted token lengths into `KVCacheRewindParams` and calls `invokeKVCacheRewind` (implemented as `computeAndInvokeKVCacheRewind` in `kv_rewind_helper.{h,cu}`), so Engineer B can invoke it without re‚Äëimplementing KV logic. Wiring this helper into `LlamaBatch` / `SequenceManager` remains a follow‚Äëup step.

10. **A10: Tighten Python‚Äëside EAGLE config/metrics and add tests**  
    - From sections 8 and 10: ensure `SpeculativeDecodingManager` / batch wrappers never run a Python draft for EAGLE, that `RequestMetrics.spec_info` is populated only from real TurboMind outputs (no fake `EngineOutput`), and add tests that exercise `SpeculativeDecodingManager` and `SpeculativeDecodingStats` for EAGLE3.

#### Engineer A ‚Äì Phase 2 (next 10 tasks, A‚Äëscope only)

Status: **[x] A10‚ÄìA20 completed**

Engineer A plan code mapping (A10‚ÄìA20):

- [x] **A10** ‚Äì item 10 above (‚ÄúTighten Python‚Äëside EAGLE config/metrics and add tests‚Äù).  
- [x] **A11** ‚Äì item 11 below (‚ÄúHarden EagleModule::load error handling and model validation‚Äù).  
- [x] **A12** ‚Äì item 12 below (‚ÄúAdd EAGLE KV rewind unit tests for `kv_rewind_helper` and `invokeKVCacheRewind`‚Äù).  
- [x] **A13** ‚Äì item 13 below (‚ÄúExpose EAGLE KV rewind metrics in RequestMetrics‚Äù).  
- [x] **A14** ‚Äì item 14 below (‚ÄúAdd a lightweight EagleModule forward microbenchmark hook‚Äù).  
- [x] **A15** ‚Äì item 15 below (‚ÄúTighten EAGLE tree / kernel invariants and logging‚Äù).  
- [x] **A16** ‚Äì item 16 below (‚ÄúImprove Python SpeculativeConfig validation for EAGLE‚Äù).  
- [x] **A17** ‚Äì item 17 below (‚ÄúDocument EagleModule / KV / metrics behaviour for offline EAGLE‚Äù).  
- [x] **A18** ‚Äì item 18 below (‚ÄúAdd configuration knobs for EAGLE debug/trace logging‚Äù).  
- [x] **A19** ‚Äì item 19 below (‚ÄúAdd safety checks around EAGLE disable/fallback paths‚Äù).  
- [x] **A20** ‚Äì item 20 below (‚ÄúPlan additional unit tests for future multi‚Äëtoken EAGLE support in A‚Äëscope‚Äù).

11. **A11: Harden EagleModule::load error handling and model validation** ‚Äì **done**  
    - Implemented in `EagleModule::load` (see `EagleModule.cc`): validates `config.yaml` fields (`hidden_units`, `vocab_size`, `head_num`, `size_per_head`, `inter_size`), checks `hidden_units == head_num * size_per_head`, enforces positivity, verifies weight file sizes via `loadTensorFromFile`, and disables EAGLE with clear `[EAGLE]` log messages on failure instead of proceeding with inconsistent weights.

12. **A12: Add EAGLE KV rewind unit tests for `kv_rewind_helper` and `invokeKVCacheRewind`** ‚Äì **done**  
    - Covered by `lmdeploy/tests/turbomind/test_speculative_kernels.py::TestKVCacheRewindHelper`: constructs synthetic block tables and draft/accepted lengths, mirrors `computeAndInvokeKVCacheRewind`‚Äôs rewind-length computation, and asserts that `invoke_kv_cache_rewind` clears exactly the expected tail blocks per slot.

13. **A13: Expose EAGLE KV rewind metrics in RequestMetrics** ‚Äì **done**  
    - `RequestMetrics` (`src/turbomind/utils/metrics.h`) now includes `eagle_total_rewound_tokens` / `eagle_rewind_steps`, exposed to Python via `_turbomind.RequestMetrics` (`src/turbomind/python/bind.cpp`). `_get_metrics` in `lmdeploy/turbomind/turbomind.py` attaches `num_rewound_tokens` / `rewind_steps` into `req_metrics.spec_info` when non‚Äëzero, and `lmdeploy/tests/turbomind/test_eagle_metrics.py` asserts that these fields are surfaced correctly.

14. **A14: Add a lightweight EagleModule forward microbenchmark hook** ‚Äì **done**  
    - Implemented as `_turbomind.eagle_forward_bench` in `src/turbomind/python/bind.cpp`, with coverage in `lmdeploy/tests/turbomind/test_eagle_module.py::test_eagle_module_forward_microbenchmark`. Runs multiple `EagleModule::forward` passes on a synthetic batch, reporting `avg_ms_per_forward` and `tokens_per_second` without touching production decode paths.

15. **A15: Tighten EAGLE tree / kernel invariants and logging** ‚Äì **done**  
    - `lmdeploy/turbomind/eagle_tree.{h,cpp}` now assert structural invariants (non‚Äëzero `max_depth_` / `max_paths_`, node count ‚â§ `max_paths_ * max_depth_`, valid node indices in paths, flattened size consistency). A small bug in `buildTreeWithChoices` was fixed to respect these invariants, ensuring misconfigured `eagle_tree.yaml` is caught early in debug builds.

16. **A16: Improve Python SpeculativeConfig validation for EAGLE** ‚Äì **done**  
    - `lmdeploy/speculative_config.py` enforces EAGLE‚Äëspecific consistency: defaults `max_path_len`, `max_decoding_tokens`, `max_non_leaves_per_layer`, `capture_layers` for `method in {"eagle", "eagle3"}`, and validates that all are positive, `max_path_len <= max_decoding_tokens`, and `max_path_len >= num_speculative_tokens`. Tests in `lmdeploy/tests/turbomind/test_eagle_spec_config_validation.py` cover invalid and valid EAGLE/EAGLE3 configurations.

17. **A17: Document EagleModule / KV / metrics behaviour for offline EAGLE** ‚Äì **done**  
    - Documented in `docs/turbomind_eagle_usage.md` (section ‚ÄúOffline EagleModule, KV, and Metrics Behaviour‚Äù), explaining how EagleModule, EAGLE KV over‚Äëprovisioning, KV rewind, and EAGLE metrics interact in TurboMind‚Äôs offline pipeline for throughput/memory tuning.

18. **A18: Add configuration knobs for EAGLE debug/trace logging** ‚Äì **done**  
    - Introduced env‚Äëvar gates for EAGLE-specific debug logging: `LMDEPLOY_EAGLE_DEBUG` (generic EAGLE traces, including `EagleModule::forward`), `LMDEPLOY_EAGLE_KV_DEBUG` (KV rewind traces in `computeAndInvokeKVCacheRewind`), and `LMDEPLOY_EAGLE_METRICS_DEBUG` (Python-side EAGLE metrics logs in `turbomind.py::_get_metrics`), so offline experiments can enable detailed traces without recompiling while defaults remain quiet.

19. **A19: Add safety checks around EAGLE disable/fallback paths** ‚Äì **done**  
    - `LlamaV2` now treats EAGLE as enabled for an engine only when a draft model path is provided and `EagleModule::load` succeeds. On missing path or failed validation, it logs a clear `[LlamaV2][EAGLE]` warning, sets `spec_mode_ = Disabled()`, and releases `eagle_module_` / `eagle_buffers_`, ensuring TurboMind cleanly falls back to baseline decoding with zero EAGLE metrics (no partial or misleading speculative stats for that engine).

20. **A20: Plan additional unit tests for future multi‚Äëtoken EAGLE support in A‚Äëscope** ‚Äì **done**  
    - Added `lmdeploy/tests/turbomind/test_eagle_multi_token_future.py` as the home for A‚Äëscope multi‚Äëtoken tests. It contains skipped test skeletons that describe the planned checks:
      - KV rewind correctness for multi‚Äëtoken steps using `EagleKVRewindConfig` / `computeAndInvokeKVCacheRewind` and synthetic block tables.
      - Aggregation of multi‚Äëtoken EAGLE metrics through `_get_metrics` and `SpeculativeDecodingStats`, ensuring invariants like `num_accepted_tokens <= num_draft_tokens` and consistent KV rewind counters.
      These tests intentionally avoid touching `LlamaBatch` / `LlamaV2_eagle` and will be filled in once Engineer B‚Äôs multi‚Äëtoken loop is integrated.

#### Engineer A ‚Äì Phase 3 (next 10 tasks, A‚Äëscope only)

21. **A21: Bind EAGLE device kernels for acceptance/mask tests** ‚Äì **üß™ prototype (GPU/CI validation pending)**  
    - Added lightweight C++/pybind11 bindings around the EAGLE CUDA kernels in `lmdeploy/lmdeploy/turbomind/kernels/speculative_decoding/common.{h,cu}` (at least `acceptDraftTokens` and `invokePackAcceptedPaths`) and introduced GPU-backed tests under `lmdeploy/tests/turbomind/test_speculative_kernels.py` that compare device results against the existing Python reference implementations for acceptance and path packing.  
    - **Scope:** Not required for baseline decode; bindings are primarily for tests/benchmarks and are not a hard dependency for production EAGLE paths (core kernels are already used from C++).  
    - **CI:**  
      - Run `pytest lmdeploy/tests/turbomind/test_speculative_kernels.py::TestAcceptDraftTokensDevice` with CUDA and a built `_turbomind` extension.  
      - Run `pytest lmdeploy/tests/turbomind/test_speculative_kernels.py::TestPackAcceptedPathsDevice` with CUDA and `_turbomind`.  
      - Run `pytest lmdeploy/tests/turbomind/test_speculative_kernels.py::TestAcceptanceStatsDevice` with CUDA and `_turbomind`.  
    - **Note:** Feature is experimental; do not rely on the Python bindings for production decode paths until the above CI jobs are green.

22. **A22: Microbenchmark EAGLE acceptance and KV rewind kernels** ‚Äì **üß™ prototype (coverage limited to small shapes)**  
    - Implemented a Python-level microbenchmark helper `benchmark_kv_cache_rewind` in `lmdeploy/turbomind/kernels/speculative_decoding/common.py` and a lightweight test in `lmdeploy/tests/turbomind/test_speculative_kernel_bench.py`. This runs repeated KV rewind operations on synthetic inputs (CPU or CUDA via torch) and reports timing statistics, giving offline tuning a cheap way to sanity-check KV rewind performance without running a full decode loop.  
    - Additional microbenchmarks for acceptance/mask kernels (`benchmark_accept_draft_tokens`, `benchmark_pack_accepted_paths`) have been added as well; these rely on `_turbomind` when running on CUDA and are validated by small sanity tests but still benefit from broader GPU/CI coverage on larger shapes.  
    - **CI:**  
      - Run `pytest lmdeploy/tests/turbomind/test_speculative_kernel_bench.py` with CUDA and `_turbomind` built, ensuring both KV rewind and acceptance/pack benchmarks run on GPU.  
      - Optionally extend CI to capture benchmark outputs (e.g. as artifacts) for basic perf regression checks.

23. **A23: Fuzz-style tests for `eagle_tree` invariants** ‚Äì **‚úÖ implemented + tested (CPU-only)**  
    - Extended `lmdeploy/tests/turbomind/test_eagle_tree.py` with fuzz-style tests (`test_random_choices_preserve_invariants` plus larger variants) that build random `choices` tables and draft token sequences and assert that `SpeculationTree::buildTreeWithChoices` / `extractPaths` always produce a flattened path array of length `num_paths * max_depth`, exercising the internal invariants under a variety of shapes.  
    - These tests are CPU-only and already run in standard CI; no GPU dependency here.

24. **A24: Guardrails for EAGLE-disabled engines in Python pipeline** ‚Äì **done (via existing metrics/benchmark tests)**  
    - Covered by the combination of `lmdeploy/tests/turbomind/test_eagle_metrics.py` (no `spec_info` when `eagle_steps == 0`), `lmdeploy/tests/test_speculative_stats.py` (no-op updates when `spec_info` is absent), and `lmdeploy/tests/test_benchmark_speculative_integration.py` (benchmark JSON only includes an `eagle_speculation` block when TurboMind provides `spec_info`). Together these ensure that when EAGLE is disabled at runtime, the Python pipeline and `BenchmarkRunner` treat runs as baseline with zero EAGLE stats.

25. **A25: EAGLE metrics summary helper for offline analysis** ‚Äì **done**  
    - Added `EagleMetricsSummary` to `lmdeploy/metrics/stats.py` plus tests in `lmdeploy/tests/test_eagle_metrics_summary.py`. This helper wraps `SpeculativeDecodingStats` into a compact summary (draft count, draft/accepted token totals, draft acceptance rate, mean acceptance length) suitable for logging or saving alongside benchmark results.

26. **A26: CLI utility to inspect `eagle_tree.yaml` and buffers** ‚Äì **‚úÖ implemented (offline-only tool)**  
    - Implemented `scripts/eagle_inspect_tree.py`, a small CLI that loads `eagle_tree.yaml`, reports basic tree statistics (node count, max/average branching), and, given `--max-decoding-tokens` / `--max-path-len` / `--batch-size`, prints derived `EagleBuffers` shapes for `draft_paths` and packed masks so engineers can reason about memory/shape implications offline.  
    - Later extended with rough KV-block usage estimates (`--block-size`) to help reason about speculative KV over-provisioning. This script is pure Python/CPU and does not depend on CUDA, but its assumptions should be revisited periodically as EAGLE buffer layouts evolve.

27. **A27: NVTX ranges for EAGLE hot paths (debug-only)** ‚Äì **done**  
    - Added NVTX scopes around key A-scope EAGLE hot paths: `EagleModule::forward` (via `NvtxScope` in `EagleModule.cc`) and the host-side KV rewind helper (`computeAndInvokeKVCacheRewind` in `kv_rewind_helper.cu`). These scopes are compatible with the existing debug env-vars and allow Nsight profiles to clearly identify EAGLE work without changing behaviour.

28. **A28: Cross-check `SpeculativeConfig` vs `EngineParam` for TurboMind** ‚Äì **done (Python-side helper + tests)**  
    - Added `SpeculativeConfig.to_turbomind_spec_dict()` and `check_turbomind_spec_alignment` in `lmdeploy/speculative_config.py`, plus tests in `lmdeploy/tests/turbomind/test_eagle_spec_config_alignment.py`. These helpers ensure that the Python `SpeculativeConfig` maps cleanly onto the engine-side `speculative_config` fields (`method`, `model`, `num_speculative_tokens`, `max_path_len`, `max_decoding_tokens`, `max_non_leaves_per_layer`) and emit a warning when a provided engine-spec dict deviates from the Python configuration.

29. **A29: Extended EAGLE failure-mode documentation and runbook** ‚Äì **done**  
    - Extended `docs/turbomind_eagle_usage.md` with a ‚ÄúEAGLE Failure Modes and Debugging Runbook‚Äù section that documents common misconfigurations (missing/invalid draft models, invalid `SpeculativeConfig`, disabled EAGLE) and how to diagnose them using the new debug env-vars, metrics, and logs.

30. **A30: Cross-backend EAGLE3 SpeculativeConfig alignment tests** ‚Äì **done (Python-level alignment)**  
    - Added `lmdeploy/tests/test_eagle_spec_config_cross_backend.py`, which constructs a single EAGLE3 `SpeculativeConfig` and asserts that:
      - It can be attached to both `PytorchEngineConfig` and `TurbomindEngineConfig` without violating their validation rules.
      - Core semantics (`method`, `num_speculative_tokens`) are identical across backends.
      - `SpeculativeConfig.to_turbomind_spec_dict()` produces a `speculative_config` mapping whose keys/values match what TurboMind‚Äôs `EngineParam` expects for EAGLE/EAGLE3.

#### Engineer A ‚Äì Future Phase (A31+ prototypes ‚Äì GPU validation pending)

31. **A31: Tree-based device acceptance kernel (paths-level)** ‚Äì **üß™ prototype (GPU/CI validation pending)**  
    - Added `invokeTreeAcceptByIdsWithPaths` in `lmdeploy/turbomind/kernels/speculative_decoding/tree_accept_kernels.{h,cu}` to evaluate SpeculationTree paths entirely on device:
      - For each sequence, walks all candidate paths in a batched `paths` tensor `[maxBatchSize, numPaths, maxPathLen]` using the same acceptance rule as the existing host-side logic in `LlamaV2_eagle.cc` (accept while `draft_id == target_id`, include the first mismatching target token).
      - Produces `best_path_ids` and `accepted_lens` per slot, and materializes accepted target tokens for the best path into `accepted_tokens`.  
    - This kernel is currently only used via test/benchmark bindings (not the production decode loop) and requires **GPU-backed unit tests in CI (including different batch sizes, `num_paths`, and `max_path_len`)** before being wired into `LlamaV2_eagle` or treated as production-ready.  
    - **Scope:** Not wired into `LlamaBatch` / `LlamaV2_eagle`; only reachable via `_turbomind` bindings and dedicated tests/benchmarks. Feature is experimental and should not be relied upon for production decode paths yet.  
    - **CI:**  
      - Build `_turbomind` with CUDA and run  
        `pytest lmdeploy/tests/turbomind/test_speculative_kernels.py::TestTreeAcceptTokensDevice`.

32. **A32: Pybind bindings and tests for tree-accept kernel** ‚Äì **üß™ prototype (GPU/CI validation pending)**  
    - Exposed the tree-accept kernel to Python as `_turbomind.eagle_tree_accept_tokens` in `src/turbomind/python/bind.cpp`, accepting CUDA tensors for `draft_ids`, `target_ids`, `paths`, `best_path_ids`, `accepted_lengths`, `accepted_tokens`, and optional `batch_slots`.  
    - Added GPU-backed tests in `lmdeploy/tests/turbomind/test_speculative_kernels.py::TestTreeAcceptTokensDevice` that compare `_turbomind.eagle_tree_accept_tokens` against a Python reference mirroring the host-side acceptance logic. These tests are guarded with `torch.cuda.is_available()` and will be **skipped** on CPU-only environments; they must be exercised in a CUDA-enabled CI configuration (with a built `_turbomind` extension) before we mark A31/A32 as fully validated.  
    - **Scope:** Binding is not used in production EAGLE paths; it exists for A-scope validation and microbenchmarks only.  
    - **CI:**  
      - Same as A31: run `pytest lmdeploy/tests/turbomind/test_speculative_kernels.py::TestTreeAcceptTokensDevice` with CUDA and `_turbomind` built.

33. **A33: Prepare kernels for variable `tokens_per_seq` layouts** ‚Äì **üß™ prototype (design scaffolding; validation pending)**  
    - The existing acceptance and packing kernels (`acceptDraftTokens`, `invokePackAcceptedPaths`, and the new tree-accept helper) are written against batched layouts with explicit `batch_slots` mappings and max-size parameters (`max_batch_size`, `max_draft_tokens`, `num_paths`, `max_path_len`), which makes them structurally compatible with future variable `tokens_per_seq` schemes (unused positions can be denoted via sentinel indices such as `-1`).  
    - No production call sites have been updated yet to drive truly per-sequence `tokens_per_seq` variation; additional work (and tests) will be needed to:
      - thread per-sequence token counts into the relevant helpers, and  
      - validate correctness under non-uniform layouts via new GPU-backed tests (once CI has a CUDA build).  
    - For now, these kernels should be considered **prototype-ready but not fully validated for variable per-sequence layouts**; downstream engineers should treat them as experimental until dedicated GPU tests (including multi-token end-to-end scenarios, TP/PP/DP configurations, and KV rewind integration) are added and passing in CI.  
    - **Scope:** No production call sites currently drive truly per-sequence `tokens_per_seq` variation; the design is in place, but wiring + tests are still TODO.  
    - **CI (future):**  
      - Add multi-token E2E tests in `lmdeploy/tests/turbomind/test_eagle_multi_token_future.py` once Engineer B‚Äôs multi-token loop is ready.  
      - Extend `test_benchmark_speculative_integration.py` to cover multi-token EAGLE runs and validate metrics/KV rewind invariants under variable `tokens_per_seq`.

34. **A34: Offline TurboMind pipeline examples for EAGLE** ‚Äì **üß™ implemented (examples; manual GPU run required)**  
    - Updated `lmdeploy/examples/speculative_decoding_example.py` to use the high-level `lmdeploy.pipeline` TurboMind backend together with `TurbomindEngineConfig` and `SpeculativeConfig` for:
      - `draft_target` speculative decoding,
      - `eagle3` EAGLE speculative decoding (including aggregation of `SpeculativeDecodingStats` and `EagleMetricsSummary` from pipeline responses), and
      - `ngram` speculative decoding.  
    - The script now demonstrates an end-to-end offline EAGLE/EAGLE3 run (kernels ‚Üí EagleModule ‚Üí TurboMind pipeline ‚Üí Python metrics) without touching `LlamaBatch` / `LlamaV2_eagle`, and makes `req_metrics.spec_info` usage explicit for EAGLE runs.  
    - **Scope:** Example-only; it assumes real TurboMind model artifacts at the placeholder paths and is intended for offline experimentation and debugging rather than CI.  
    - **CI / testing:**  
      - Syntax/packaging is covered by `python -m compileall lmdeploy/examples/speculative_decoding_example.py`.  
      - Full runs should be exercised on a CUDA-enabled machine with small TurboMind models by engineers or future GPU CI (no dedicated pytest currently).

35. **A35: Print EAGLE metrics in `benchmark_speculative.py` CLI** ‚Äì **‚úÖ implemented (leverages existing JSON tests)**  
    - Extended `inference/benchmark_speculative.py::BenchmarkRunner.run_test_scenario` to print `mean_acceptance_rate` and `mean_acceptance_length` whenever the `eagle_speculation` block is present in the benchmark results, so offline TurboMind benchmarks surface EAGLE acceptance behaviour directly in stdout alongside throughput/latency/memory.  
    - This keeps the JSON schema unchanged (the `eagle_speculation` block is still produced by `EagleMetricsSummary.to_dict()` plus an `enabled` flag) and simply makes EAGLE metrics more visible during manual benchmarking.  
    - **Scope:** Purely a reporting enhancement; no changes to engine wiring, kernels, or metrics computation.  
    - **CI / testing:**  
      - Structural/semantic checks for the `eagle_speculation` block remain covered by `lmdeploy/tests/test_benchmark_speculative_integration.py::test_benchmark_runner_reports_eagle_metrics_when_available` (run in CUDA-enabled CI with `MODEL_PATH` / `SPEC_MODEL_PATH` set).  
      - The new print path is best-effort and does not require additional automated tests.

36. **A36: Harden core EAGLE kernels for edge cases** ‚Äì **üß™ prototype (GPU/CI validation pending)**  
    - Tightened the EAGLE acceptance, packing, tree-accept, and KV-rewind kernels in `lmdeploy/turbomind/kernels/speculative_decoding/common.{h,cu}` and `tree_accept_kernels.{h,cu}` to handle edge conditions more robustly:
      - `acceptDraftTokensKernel` now guards against out-of-range `batch_slots`, negative `best_path_id` (treated as "no best path"), and path indices outside `[0, max_draft_tokens)`, defaulting to zero accepted tokens for such slots instead of reading out of bounds.  
      - `packAcceptedPathsKernel` now validates `batch_slots` against `max_batch_size`, skips slots with invalid `best_path_id` or non-positive `accepted_len`, and clamps writes to the valid range implied by `accepted_lengths_cumsum`, treating negative path indices as terminators.  
      - `kvCacheRewindKernel` now uses `max_batch_size` to ignore out-of-range slots and early-returns when batch size, block size, or max_blocks_per_seq are nonsensical, avoiding stray writes to block_tables/kv_cache_blocks.  
      - A new GPU test in `lmdeploy/tests/turbomind/test_speculative_kernels.py::TestAcceptDraftTokensDevice::test_device_accept_handles_empty_paths_and_negative_best_path` validates that `best_path_id == -1` and all-`-1` paths result in zero accepted tokens and unchanged sequence lengths.  
    - **Scope:** Kernel behaviour remains backward compatible for valid inputs; changes only add guards for malformed or partially-initialized buffers commonly hit during experimentation.  
    - **CI / testing:**  
      - Run `pytest lmdeploy/tests/turbomind/test_speculative_kernels.py::TestAcceptDraftTokensDevice` and `::TestPackAcceptedPathsDevice` on CUDA with a built `_turbomind` extension to exercise the hardened kernels on device.  
      - Add future GPU tests for KV rewind edge cases once Engineer B wires `computeAndInvokeKVCacheRewind` into decode loops.

37. **A37: Expose EagleModule buffer / tree introspection helpers** ‚Äì **‚úÖ implemented (internal utility)**  
    - Extended `EagleModule` with additional read-only accessors used by A-scope tooling and potential future buffer sizing helpers:
      - `getHiddenUnits()` and `getVocabSize()` expose the draft model dimensions inferred from `config.yaml`.  
      - `getMaxTreeNodes()` reports `max_decoding_tokens * max_draft_path_len`, a conservative upper bound on per-step tree nodes for sizing SpeculationTree/EagleBuffers.  
    - Strengthened `EagleModule::forward` checks:
      - If `enabled_` is false, `forward` now logs a clear warning and acts as a pass-through on `hidden_states` instead of attempting a partial draft forward.  
      - Existing hidden-dim mismatch warnings remain, but now run under an explicit "module disabled / weights not initialized" guard for cleaner failure modes.  
    - **Scope:** No behaviour changes for correctly configured engines; the new accessors are used only by diagnostics and potential future tooling.  
    - **CI / testing:** Covered implicitly by existing EAGLE unit/integration tests that exercise `EagleModule::load`/`forward` via TurboMind; no additional tests required in this pass.

38. **A38: SpeculativeConfig/EAGLE runtime validation helper** ‚Äì **‚úÖ implemented (warning-only, offline use)**  
    - Added `validate_eagle_runtime_config(engine_config, spec_cfg)` to `lmdeploy/speculative_config.py`. This helper performs non-fatal runtime checks for EAGLE/EAGLE3 configurations:
      - Verifies that a non-None `SpeculativeConfig` with `method in {"eagle", "eagle3"}` is paired with an engine_config that has a `speculative_config`.  
      - When an engine-side speculative config object is available, it attempts to obtain a dict (via `to_turbomind_spec_dict` or `__dict__`) and calls `check_turbomind_spec_alignment` to warn on drifts between Python and engine settings.  
      - Emits a warning when `engine_config.enable_metrics` is False, since this would suppress EAGLE metrics on `req_metrics.spec_info`.  
    - **Scope:** Helper is opt-in and side-effect free with respect to decode logic; it is intended for offline inspection tools, examples, and debug scripts.  
    - **CI / testing:** Basic import/compile sanity is covered; callers should add targeted tests around warning behaviour as needed.

39. **A39: Offline TurboMind EAGLE inspect helper (Python)** ‚Äì **üß™ implemented (requires GPU / real models)**  
    - Introduced `lmdeploy/turbomind/eagle_inspect.py::inspect_offline_eagle`, a small A-scope utility that:
      - Builds a TurboMind pipeline via `lmdeploy.pipeline` with a `TurbomindEngineConfig(speculative_config=SpeculativeConfig(method="eagle3", ...))`.  
      - Uses `SpeculativeDecodingStats` + `EagleMetricsSummary` to aggregate `req_metrics.spec_info` across responses.  
      - Prints the resulting `EagleMetricsSummary.to_dict()` and returns it as a dict.  
      - Calls `validate_eagle_runtime_config` to surface obvious misconfigurations without affecting engine behaviour.  
    - **Scope:** Purely an offline helper; it does not alter any decode paths or B-scope integration. Intended for engineers running small EAGLE-capable models on GPU.  
    - **CI / testing:**  
      - Syntax is checked via `python -m compileall lmdeploy/lmdeploy/turbomind/eagle_inspect.py`.  
      - Full runs should be exercised manually or in a GPU CI job configured with `MODEL_PATH` / `SPEC_MODEL_PATH`; no automated pytest currently drives this helper.

40. **A40: EAGLE tree/CLI what-if enhancements** ‚Äì **‚úÖ implemented (offline-only tooling)**  
    - Extended `scripts/eagle_inspect_tree.py` to provide richer guidance for tuning `SpeculativeConfig` against a given `eagle_tree.yaml`:
      - Computes and prints the observed maximum path depth from `SpeculationTree.getPathsFlat()` and the maximum number of non-leaf nodes per level via `getNonLeafNodes(level)`, alongside existing branching stats.  
      - Adds `--num-spec-tokens` and `--what-if` flags. In what-if mode, the script reports recommended structural fields for EAGLE/EAGLE3:
        - `num_speculative_tokens` (from `--num-spec-tokens`),  
        - recommended `max_path_len` (observed depth),  
        - recommended `max_non_leaves_per_layer` (max non-leaf count per level), and
        - the current `--max-decoding-tokens` treated as `SpeculativeConfig.max_decoding_tokens`.  
      - Emits a warning when `num_speculative_tokens` exceeds the observed tree depth, indicating that the tree cannot represent that many speculative tokens per step.  
      - Keeps and documents the existing rough KV-block usage estimates based on `--block-size`, now clearly labelled as speculative-only and approximate.  
    - **Scope:** Script remains CPU-only and does not depend on TurboMind; it is intended for offline reasoning about tree/config/KV trade-offs.  
    - **CI / testing:**  
      - Syntax sanity via `python -m compileall scripts/eagle_inspect_tree.py`.  
      - Behaviour is validated manually by engineers using sample `eagle_tree.yaml` files and SpeculativeConfig-like parameters.

### Engineer B (this plan) ‚Äì LlamaBatch / LlamaV2_eagle / sampling / integration tests

Phase 1 (completed) ‚Äì initial wiring and one-step acceptance:

- [x] Use EAGLE engine budget in `LlamaBatch` logs.  
- [x] Add speculative branch skeleton in `LlamaBatch::Forward`.  
- [x] Capture per‚Äësequence hidden states for draft generation.  
- [x] Call `EagleModule::forward` (via `LlamaV2::eagleDraftForward`) and sample `draft_tokens` in `LlamaBatch`.  
- [x] Wire `draft_tokens` into `LlamaV2::eagleSpeculativeStep`.  
- [x] Mirror one‚Äëstep accepted tokens/lengths into `EagleBuffers::outputs` on device.  
- [x] Implement one‚Äëstep target‚Äëverify path in `LlamaV2_eagle` and log real acceptance rates.  
- [x] Tie acceptance to decode state by checking post‚Äë`dynamicDecode` tokens in `LlamaBatch`.  
- [x] Add EAGLE vs baseline integration test skeleton in `lmdeploy/tests/test_eagle_e2e.py`.  
- [x] Wire `inference/benchmark_speculative.py` for baseline vs EAGLE comparisons.

Phase 2 (next 10 Engineer‚ÄëB tasks, all avoiding Engineer A‚Äôs EagleModule/KV/Python‚Äëwrapper work):

- [x] **Design multi‚Äëtoken EAGLE engine loop in `LlamaBatch`**  
  - Use `eagle_max_engine_tokens_per_step_` and a per‚Äësequence helper (`compute_eagle_draft_tokens_per_seq`) to define how many draft/engine tokens per step EAGLE mode should attempt, keeping per‚Äësequence engine shapes as static as possible for future CUDA‚Äëgraph capture. This is currently logged per step without yet changing decode behaviour.

- [x] **Implement multi‚Äëtoken `draft_tokens` layout in `LlamaBatch`**  
  - Extend the speculative branch to build a flattened host layout `[batch_size, tokens_per_seq]` for draft and target tokens (currently populated with one token per sequence) and pass `num_draft_tokens = batch_size * tokens_per_seq` into `LlamaV2::eagleSpeculativeStep`, so increasing `tokens_per_seq > 1` later does not change the interface.

- [x] **Extend `LlamaV2_eagle` for multi‚Äëtoken tree mapping (layout‚Äëready)**  
  - Update `LlamaV2::eagleSpeculativeStep` to interpret `draft_tokens` as `[batch_size, tokens_per_seq]`, derive `tokens_per_seq` from `num_draft_tokens / batch_size`, and expand the single‚Äësequence `SpeculationTree` paths into a fully batched `[batch_size, max_decoding_tokens, max_path_len]` layout in `EagleBuffers::inputs.draft_paths`. One draft token per sequence is still used for acceptance, but the tree/mask layout is now multi‚Äëtoken ready.

- [x] **Wire `acceptDraftTokens` / `invokePackAcceptedPaths` for multi‚Äëtoken acceptance (logic‚Äëonly, pack side)**  
  - Call the speculative decoding helpers `acceptDraftTokens` and `invokePackAcceptedPaths` from `LlamaV2_eagle` so that, for the current one‚Äëtoken‚Äëper‚Äësequence regime, `EagleBuffers::outputs.accepted_tokens` / `accepted_lens` are produced entirely on device (using a per‚Äësequence linear path layout) and `accepted_lengths_cumsum` / `accepted_path_offsets` are populated for downstream KV / decode updates. Full tree‚Äëbased multi‚Äëtoken acceptance remains to be implemented.
  - Add per‚Äësequence `[LlamaBatch][EAGLE]` debug logs (draft/target/accepted_len) in `LlamaBatch::Forward` so that device‚Äëside acceptance decisions are visible when debugging multi‚Äëtoken behaviour.

- [ ] **Advance sequences by accepted tokens in `LlamaBatch` (no KV rewind)**  
  - Use `accepted_tokens` / `accepted_lens` (and the packed offsets) from `EagleBuffers` to update `token_ids_buf_` and `sequence_lengths_` for multi‚Äëtoken steps in `LlamaBatch::Forward`, ensuring correctness when KV rewind is still a no‚Äëop.

- [ ] **Adapt or bypass `DynamicDecodeLayer` for EAGLE multi‚Äëtoken mode**  
  - Decide whether EAGLE multi‚Äëtoken steps should bypass `DynamicDecodeLayer` entirely (doing sampling in `LlamaBatch`) or extend it to understand multi‚Äëtoken acceptance, and implement the chosen strategy without touching KV cache code.

- [x] **Add deterministic baseline‚Äëvs‚ÄëEAGLE equality test (num_spec_tokens=1)**  
  - In `lmdeploy/tests`, add an integration test that runs TurboMind with and without EAGLE (with `num_speculative_tokens=1`) on a small model and fixed seed, asserting identical token outputs.

- [x] **Add acceptance‚Äërate sanity test using logs or metrics**  
  - Add a test that runs EAGLE on a small scenario and inspects `RequestMetrics.spec_info` (or logs/metrics) to assert that speculative metrics are present and that accepted tokens never exceed draft tokens.

- [x] **Enhance `benchmark_speculative.py` to report EAGLE acceptance metrics**  
  - Extend the benchmark script to consume TurboMind EAGLE metrics (via `RequestMetrics.spec_info`) and include acceptance statistics (draft/accepted tokens, acceptance rate) in the saved JSON results for speculative runs.

- [x] **Document TurboMind EAGLE usage, tests, and benchmarks**  
  - Add a short doc section describing how to:
    - enable TurboMind EAGLE with `SpeculativeConfig`,
    - run `test_eagle_e2e.py`,
    - and run `inference/benchmark_speculative.py` for baseline vs EAGLE comparisons.
  - Implemented as `docs/turbomind_eagle_usage.md`, which covers configuration, tests, and benchmark usage, including how to interpret the `eagle_speculation` metrics block in benchmark outputs.

Engineer B plan code mapping (EngineerB‚Äë01‚Ä¶EngineerB‚Äë20):

- [x] **EngineerB‚Äë01: use eagle engine tokens**  
  - Mapped to Section 4‚Äôs EAGLE‚Äëaware engine token budget and the Phase‚Äë1 bullet ‚ÄúUse EAGLE engine budget in `LlamaBatch` logs.‚Äù  
- [x] **EngineerB‚Äë02: multi‚Äëtoken draft layout invariants**  
  - Mapped to Phase‚Äë2 task ‚ÄúImplement multi‚Äëtoken `draft_tokens` layout in `LlamaBatch`.‚Äù  
- [x] **EngineerB‚Äë03: map multi‚Äëtoken drafts to trees**  
  - Mapped to Phase‚Äë2 task ‚ÄúExtend `LlamaV2_eagle` for multi‚Äëtoken tree mapping (layout‚Äëready).‚Äù  
- [x] **EngineerB‚Äë04: wire `acceptDraftTokens` device acceptance**  
  - Mapped to Phase‚Äë2 task ‚ÄúWire `acceptDraftTokens` / `invokePackAcceptedPaths` for multi‚Äëtoken acceptance (logic‚Äëonly, pack side).‚Äù  
- [x] **EngineerB‚Äë05: advance sequences by accepted tokens**  
  - Implemented by wiring EAGLE acceptance into `LlamaBatch::Forward` after `dynamicDecode`: per‚Äësequence `accepted_lens` / `accepted_tokens` from `LlamaV2::eagleSpeculativeStep` are compared against the actual token committed by `DynamicDecode`, and `RequestMetrics.eagle_total_accepted_tokens` is advanced only when the accepted token matches the committed decode token. This keeps speculative metrics aligned with the real decode state in the current single‚Äëtoken regime, and lays the groundwork for true multi‚Äëtoken sequence advancement once the inner EAGLE loop is extended beyond `tokens_per_seq == 1`.  
- [ ] **EngineerB‚Äë06: adapt `DynamicDecodeLayer` for EAGLE (in progress)**  
  - Mapped to Phase‚Äë2 task ‚ÄúAdapt or bypass `DynamicDecodeLayer` for EAGLE multi‚Äëtoken mode.‚Äù Current work focuses on threading EAGLE acceptance through the decode step so that only tokens consistent with `dynamicDecode` are ever treated as accepted; full multi‚Äëtoken integration with `DynamicDecodeLayer` remains to be completed.
- [x] **EngineerB‚Äë07: keep single‚Äëtoken EAGLE semantics**  
  - Backed by the deterministic equality test and one‚Äëtoken semantics in `lmdeploy/tests/test_eagle_e2e.py::test_eagle_equals_baseline_single_token`.  
- [ ] **EngineerB‚Äë08: add multi‚Äëtoken tests (Engineer C infra)**  
  - Planned in Section 12 (Engineer C) as ‚ÄúExtend tests for multi‚Äëtoken EAGLE decoding once implemented.‚Äù  
- [ ] **EngineerB‚Äë09: validate benchmarks with multi‚Äëtoken EAGLE**  
  - To be built on top of `lmdeploy/tests/test_benchmark_speculative_integration.py` once multi‚Äëtoken EAGLE decode is wired.  
- [x] **EngineerB‚Äë10: document remaining gaps/limits**  
  - Covered by `docs/turbomind_eagle_usage.md`, including notes on current single‚Äëtoken and multi‚Äëtoken limitations.  
- [x] **EngineerB‚Äë11: honor `eagleMaxEngineTokensPerStep`**  
  - Implemented by using `eagle_max_engine_tokens_per_step_` in `LlamaBatch::Forward` to bound the planned `tokens_per_seq` per decode mini-batch, and reflecting this budget in the EAGLE logging so that future multi-token inner loops can respect a static engine-token budget per step.
- [ ] **EngineerB‚Äë12: support variable `tokens_per_seq` shapes**  
  - Will extend the current flattened `[batch_size, tokens_per_seq]` layout to handle per‚Äësequence variability once multi‚Äëtoken decode is implemented.  
- [ ] **EngineerB‚Äë13: add detailed EAGLE acceptance logs**  
  - Initial per‚Äësequence `[LlamaBatch][EAGLE]` logs are present; additional structured logs for multi‚Äëtoken acceptance remain to be added.  
- [x] **EngineerB‚Äë14: keep acceptance metrics consistent**  
  - Backed by `test_eagle_acceptance_metrics_sanity` in `lmdeploy/tests/test_eagle_e2e.py` and the metrics plumbing tests in `lmdeploy/tests/turbomind/test_eagle_metrics.py`.  
- [ ] **EngineerB‚Äë15: reduce host‚Äëdevice copies**  
  - Future optimization work around `draft_tokens`, paths, and acceptance buffers once correctness is fully validated.  
- [ ] **EngineerB‚Äë16: keep DP/TP layouts compatible**  
  - Future validation work to ensure multi‚Äëtoken EAGLE paths remain correct under data/pipeline parallelism.  
- [ ] **EngineerB‚Äë17: add multi‚Äëtoken fallback path**  
  - Planned safety path to drop back to single‚Äëtoken or baseline decoding when multi‚Äëtoken invariants are violated.  
- [ ] **EngineerB‚Äë18: coordinate KV rewind hook interface**  
  - Will connect Engineer‚ÄëA‚Äôs `computeAndInvokeKVCacheRewind` helper into `LlamaBatch` once multi‚Äëtoken acceptance is wired.  
- [ ] **EngineerB‚Äë19: align `DynamicDecodeLayer` stop criteria**  
  - Future work to ensure stopping criteria are consistent between baseline and EAGLE multi‚Äëtoken modes.  
- [ ] **EngineerB‚Äë20: summarize Engineer‚ÄëB work in TODO**  
  - This Engineer‚ÄëB section of `EAGLE_TODO.md` now serves as the living summary; additional updates may refine it as multi‚Äëtoken work progresses.

## 12. Agent test coverage and follow‚Äëup plan (Codex CLI) - ENGINEER C

This section tracks tests and harness work added by the Codex CLI agent to
validate the Python‚Äëside EAGLE integration and to map TODO items to concrete
test modules.

- [x] Map existing EAGLE TODO items to tests  
  - Section 2 / Engineer A item 7 (‚ÄúAdd focused unit tests for `eagle_tree`‚Äù) is covered by `lmdeploy/tests/turbomind/test_eagle_tree.py`.  
  - Section 8 / Engineer A item 6 (‚ÄúExport and plumb TurboMind EAGLE acceptance metrics end‚Äëto‚Äëend‚Äù) is covered on the Python side by `lmdeploy/tests/turbomind/test_eagle_metrics.py`.  
  - Section 10 / Engineer A item 10 (‚ÄúTighten Python‚Äëside EAGLE config/metrics and add tests‚Äù) is covered by `lmdeploy/tests/turbomind/test_speculative_manager_eagle.py` and `lmdeploy/tests/turbomind/test_speculative_manager_eagle_batch.py`.  
  - Engineer B Phase‚Äë1 integration tests (‚Äúbaseline vs EAGLE equality‚Äù and ‚Äúacceptance‚Äërate sanity‚Äù) are covered by `lmdeploy/tests/test_eagle_e2e.py`.

- [x] Add benchmark integration test for EAGLE metrics (Engineer B task)  
  - Added `lmdeploy/tests/test_benchmark_speculative_integration.py::test_benchmark_runner_reports_eagle_metrics_when_available` to exercise `inference/benchmark_speculative.py` together with a real TurboMind pipeline.  
  - This test asserts that when TurboMind populates `RequestMetrics.spec_info`, the benchmark results JSON includes an `eagle_speculation` block with sane invariants (`enabled == True`, `total_accepted_tokens <= total_draft_tokens`, acceptance rate within `[0, 1]`).

- [x] Add SpeculativeDecodingStats unit tests for EAGLE metrics aggregation  
  - Added `lmdeploy/tests/test_speculative_stats.py` to validate that `SpeculativeDecodingStats.update_from_output` correctly consumes TurboMind EAGLE metrics from `EngineOutput.req_metrics.spec_info`, updates draft/accepted token counters, and leaves stats unchanged when `spec_info` is absent.

- [ ] Extend tests for multi‚Äëtoken EAGLE decoding once implemented  
  - When Section 4 multi‚Äëtoken tasks and Engineer B Phase‚Äë2 items are implemented, add tests that:  
    - drive multi‚Äëtoken speculative steps end‚Äëto‚Äëend via `LlamaBatch::Forward`,  
    - verify that `accepted_tokens` / `accepted_lens` advance sequences correctly without KV rewind,  
    - and ensure `RequestMetrics.spec_info` reflects multi‚Äëtoken acceptance statistics.

- [ ] Harden CI configuration for EAGLE‚Äëspecific tests  
  - Provide small TurboMind models and CI env configuration (`MODEL_PATH` / `SPEC_MODEL_PATH`) so that all Python‚Äëside EAGLE tests (metrics, managers, benchmark integration, e2e) run without manual setup and without relying on large production models.
