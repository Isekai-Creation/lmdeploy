# TurboMind EAGLE3 – Remaining Coding Tasks for TensorRT‑LLM‑Style Multi‑Token Speculative Decoding

This file lists **only implementation / wiring tasks** still required for
TurboMind’s multi‑token EAGLE/EAGLE3 to behave like TensorRT‑LLM. There are
no test or documentation items here.

Statuses:
- `[ ]` pending / not implemented yet

---

## 0. Progress Snapshot (approximate)

These are coarse‑grained progress estimates for the major areas:

- Target‑tree decode + KV + masks: **~85–90% complete**
  - All core pieces are implemented and guarded; remaining work is mostly tuning and SpecPV integration.
- Acceptance logic + KV rewind + multi‑token tails: **~80–90% complete**
  - Semantics match TensorRT‑LLM (ID‑equality, KV rewind, tail commit); further refinements are around metrics and edge‑cases only.
- Eagle3 converter + geometry + capture/FC path: **~85–95% complete**
  - Geometry and weight export are correct; capture layers and FC+norm are structurally aligned with Eagle‑3.
- Eagle3 draft logits (attention + FFN numerics): **~20–30% complete**
  - Current draft head is still a shallow “attention‑like” block; logits are far from HF/TRT, and acceptance remains ≈1.
- SpecPV partial‑KV verification: **~20% complete**
  - Kernels and scaffolding exist; not yet wired into the production EAGLE3 target‑tree path.

The remaining high‑impact work is concentrated in **Section 4.2** (Eagle3 draft head & numerics) and **Section 5** (SpecPV), which must be completed to reach full TensorRT‑LLM parity and practical performance.

---

## 1. Target‑Tree Decode & Target Verification (core C++/CUDA)

These tasks make the target model actually verify the EAGLE tree, not just
the next token. They are the main blockers for TensorRT‑LLM parity.

### 1.1 Base‑model target‑tree decode

- [x] **[A] Add explicit tree‑decode entry point**
  - Introduce a `runEagleTargetTreeDecode(int batch_size)` (or similarly
    named) method on `LlamaV2` that:
    - Assumes `EagleBuffers::inputs.eagle_net_input_ids`,
      `.eagle_net_position_ids`, `.eagle_net_hidden_indices`,
      `.eagle_net_gen_lens` are already filled by `targetTreeDecode()`.
    - Executes a dedicated tree decode pass and leaves logits / IDs in
      EAGLE buffers for acceptance.
  - Call this from `LlamaBatch::Forward` / `LlamaV2_eagle::eagleSpeculativeStep`
    after tree + masks + `targetTreeDecode()` and before calling the
    acceptance kernel.

- [x] **[B] Implement scratch‑KV tree decode pass**
  - Reuse prefix KV from `SequenceManager` as read‑only:
    - Use the same `kv_block_ptrs_`, `cu_block_nums_`, and
      `sequence_lengths_` inputs as baseline decode.
  - Allocate a scratch KV region for tree tokens:
    - Either by reserving blocks per sequence in `BlockManager`, or by
      extending existing KV helpers with a “scratch blocks” table that is
      never attached to `Sequence::blocks`.
  - Drive `UnifiedDecoder` on the flattened tree tokens:
    - Use `eagle_net_input_ids` / `eagle_net_position_ids` instead of
      `input_ids_buf_`.
    - Use `eagle_net_gen_lens` and `sequence_lengths_` to build the
      equivalent of `h_input_length_buf_` / `h_context_length_buf_` for the
      tree pass.
  - Ensure all K/V writes during this pass go to the scratch KV blocks and
    are discarded after acceptance (no mutation of live `Sequence::blocks`).

- [x] **[C] Add tree‑decode logits buffer and call `postDecodeEmbedding`**
  - Extend `EagleBuffers` or `LlamaV2` to hold a dedicated logits buffer for
    tree tokens, e.g. `Tensor eagle_tree_logits` or a raw
    `[num_tree_tokens, vocab_size_padded]` buffer.
  - After the scratch tree decode `Forward`, call `LlamaV2::postDecodeEmbedding`
    with the tree hidden states to fill this logits buffer, just like the
    main decode path does for the current step.

### 1.2 Tree masks in attention

- [x] **[C] Integrate packed tree masks into `UnifiedAttentionLayer`**
  - Add optional arguments / state so that `UnifiedAttentionLayer` can
    consume a per‑token packed mask derived from
    `EagleBuffers::inputs.packed_masks` when running the tree decode pass.
  - For the tree decode path:
    - Ensure Q/K/V computation uses these masks so that tree nodes attend
      only to allowed ancestors / branches (similar to TRT‑LLM’s
      `eagleDecodingKernels` behaviour).
  - Keep the baseline decode path unchanged when no mask is provided.

### 1.3 Logits → per‑node `target_tokens`

- [x] **[B] Implement device‑side argmax → `target_tokens` scatter**
  - Given tree logits `[num_tree_tokens, vocab_size_padded]` and
    `eagle_net_hidden_indices` `[num_tree_tokens, 2]` (slot, token_idx):
    - Launch a kernel that:
      - Computes `argmax_vocabulary` per row,
      - Maps row index → `(slot, token_idx)`,
      - Writes the resulting ID into
        `EagleBuffers::inputs.target_tokens[slot * max_decoding_tokens + token_idx]`.
  - Keep layout and dtype consistent with
    `invokeTreeAcceptByIdsWithPaths` expectations (`int32` IDs, flattened
    `[max_batch_size, max_decoding_tokens]`).

### 1.4 Wire acceptance to real per‑node target IDs

- [x] **[A] Swap acceptance over to tree‑decoded `target_tokens`**
  - Update `LlamaV2::eagleSpeculativeStep` so that, when
    `enable_eagle_target_tree == true`:
    - It assumes `inputs.target_tokens` has been filled by the tree decode
      argmax path (not by host‑fabricated next‑step IDs).
    - It calls `invokeTreeAcceptByIdsWithPaths` using:
      - `draft_ids  = inputs.draft_tokens`,
      - `target_ids = inputs.target_tokens`,
      - `paths      = inputs.draft_paths`,
      - `batch_slots` as currently wired.
  - Keep the old host‑fabricated `target_tokens` path as a fallback when
    tree decode fails a runtime check or is disabled.

### 1.5 BF16 / MXFP4 correctness for tree decode

- [x] **[C] Enforce BF16 compute and MXFP4 weight usage**
  - Ensure the tree decode path:
    - Uses BF16 activations (matching GPT‑OSS compute) and MXFP4 weights for
      GPT‑OSS‑120B just like the baseline decode path.
    - Allocates the tree logits buffer with a float type appropriate for
      stable argmax (e.g. FP32).
  - Audit dtype usage in:
    - Tree hidden‑state buffers passed into `postDecodeEmbedding`,
    - Tree logits → `target_tokens` scatter kernel,
    - Any new scratch KV structures introduced for tree decode.

---

## 2. Multi‑Token Engine & DynamicDecode Parity

These tasks close behavioural gaps between multi‑token EAGLE3 and baseline
`DynamicDecode` semantics so that multi‑token steps are “first‑class citizens”
instead of bolted‑on.

- [x] **[B] Finalize integration with `DynamicDecodeLayer`**
  - Current state:
    - `LlamaV2::dynamicDecodeWithSpec` is wired and receives `decoder_features`
      and a populated `SpecContext` (per‑slot sequences, device sequence
      lengths, planned draft tokens, EAGLE flags).
    - It still delegates all speculative work to the existing EAGLE glue in
      `LlamaBatch::Forward` / `LlamaV2_eagle::eagleSpeculativeStep`; EAGLE
      draft, tree decode, acceptance, multi‑token commit, and KV rewind all
      happen outside `dynamicDecodeWithSpec`.
  - Final integration strategy (to be implemented in a later iteration):
    - Keep `DynamicDecodeLayer` as the single‑token base decode.
    - Use `dynamicDecodeWithSpec` only to fuse *target‑tree decode* and
      *acceptance* around that base step:
        - Run `dynamicDecode(...)` for the base token.
        - Inside `dynamicDecodeWithSpec`, use `decoder_features`,
          `SpecContext` and existing helpers to:
            - run EAGLE draft + target‑tree decode on device,
            - call `invokeTreeAcceptByIdsWithPaths`,
            - expose per‑step acceptance results to `LlamaBatch`.
    - Leave multi‑token append (`advanceSequencesByEagleAcceptance`) and KV
      rewind (`runEagleKVRewind`) in `LlamaBatch`, which already has access
      to `Request` / EOS / `max_new_tokens` state.
  - Ensure that in multi‑token EAGLE mode:
    - `output_ids`, `finished`, `sequence_length` seen by callers reflect
      the accepted tokens (base + extras), not just the first token.

- [x] **[A] Clean up multi‑token advancement and `g.step` handling**
  - Tighten `advanceSequencesByEagleAcceptance` and
    `runEagleMultiTokenAdvance` so that:
    - The step counter `g.step` and the time axis in `token_ids_buf_` are
      updated consistently for multi‑token steps.
    - No sequence can get out of sync between:
      - The committed dynamic token for the step, and
      - The extra tokens appended via EAGLE acceptance.

- [x] **[C] Align EOS / stop / `max_new_tokens` semantics**
  - Refine the logic in multi‑token advancement and per‑slot kill switches
    so that:
    - EOS is never “skipped over” by extra accepted tokens.
    - Per‑request stop conditions (EOS, `max_new_tokens`, other stop
      criteria) match baseline behaviour even when multiple tokens are
      accepted in one step.
  - Keep the implementation conceptually aligned with TensorRT‑LLM’s
    `EagleDecodingLayer` stop/accept logic wherever possible.

---

## 3. Performance, Parallelism, and Clean‑up

These tasks are not strictly required for correctness but are important for
making the implementation practical and robust in production.

- [x] **[B] Reduce host↔device copies in EAGLE paths**
  - Identify and remove avoidable host/device memcpy in:
    - Draft sampling (`draft_logits` / `eagle_sampling_logits`),
    - Tree construction and acceptance,
    - Target‑tree decode (logits and `target_tokens`).
  - Move as much sampling / argmax / acceptance work as possible to device
    kernels, keeping host involvement minimal.

- [x] **[C] Make multi‑token EAGLE behaviour explicit under TP/DP/PP**
  - Decide how multi‑token EAGLE should behave when:
    - `tp_size_ > 1`,
    - Data or pipeline parallelism is enabled.
  - Implement one of:
    - A fully correct multi‑token path under these topologies (mirroring
      the single‑GPU behaviour), or
    - A clear, early runtime gate that disables multi‑token EAGLE and
      logs a reason when TP/DP/PP are active.

---

## 4. DynamicDecode Multi‑Position Parity (Advanced TRT‑LLM Alignment)

These items push TurboMind toward **full DynamicDecode‑level parity** with
TensorRT‑LLM’s EAGLE‑3 implementation. They assume sections 1–3 are already
functionally complete.

### 4.1 Tail semantics inside `DynamicDecodeLayer::ForwardMultiStep`

- [x] **[B] Apply full stop criteria to tail tokens**
  - Extend `ForwardMultiStep` so that each committed tail token is checked
    with the same logic as the base token:
    - EOS / multiple `eos_ids`,
    - stop‑words / bad‑words,
      - `max_new_tokens` / per‑slot length limits,
      - guidance / repetition‑penalty related state if present.
  - When a tail token triggers EOS/stop for a slot:
    - Mark that slot as finished,
    - Stop committing further tail tokens for that slot.

- [x] **[B] Drive KV/metrics strictly from committed tail lengths**
  - Use `ForcedTailContext::committed_lengths[i]` as the *only* source of
    truth for how many extras were actually written for slot `i`.
  - Ensure `updateEagleMetricsAndKVLengths` and `runEagleKVRewind` derive
    `kv_draft_lengths` / `kv_accepted_lengths` and all EAGLE metrics from:
      - `1 + committed_lengths[i]` (base + committed extras),
    - not from planned lengths or raw accepted lens before tail clamping.

### 4.2 EAGLE3 draft head & numerics (EagleModule parity)

- [ ] **[B] Promote Eagle3 draft to a real decoder layer (full TensorRT‑style)**
  - Implement a true Eagle3 draft layer inside TurboMind using existing
    LLaMA attention/FFN primitives, mirroring
    `tensorrt_llm/_torch/models/modeling_speculative.py::Eagle3DecoderLayer`
    and `Eagle3DraftModel`:
    - Create `Eagle3DraftLayerWeight` that owns:
      - `LlamaAttentionWeight` built from Eagle3’s fused `w_qkv` and `wo`
        with correct head geometry (head_num, kv_head_num, size_per_head).
      - `LlamaFfnWeight` built from merged `w1/w3` (gate+up) and `w2`
        (down), using the same activation (e.g. silu) as GPT‑OSS Eagle3.
      - Norms: `input_norm` (optional), `hidden_norm` (pre‑attn),
        `post_attn_norm` (ffn_norm), `output_norm` (final).
    - Add `Eagle3DraftLayer::Forward` that:
      - Applies `hidden_norm` to the draft hidden input.
      - Optionally normalizes embeddings via `input_layernorm` and
        concatenates `[embeds_norm, hidden_norm]` along the feature axis
        when Eagle3 config requires it.
      - Runs full multi‑head attention via `LlamaAttentionLayer` /
        `UnifiedAttentionLayer` with `AttentionParams` (Q/K/V split,
        softmax over sequence, RoPE, KV cache).
      - Applies post‑attention RMSNorm with residual (like TRT
        `post_attention_layernorm`).
      - Runs `LlamaFfnLayer` (GatedMLP) and then applies `output_norm` to
        produce the draft hidden before LM head.
    - Wire this into `EagleModule::load`:
      - Parse Eagle3 geometry (`eagle_q_size`, `eagle_kv_size`,
        `head_num`, `kv_head_num`, `size_per_head`, `intermediate_size`,
        `num_capture_layers`) from config.yaml produced by the converter.
      - Construct `Eagle3DraftLayerWeight` and ensure all shapes match the
        exported Eagle3 tensors.
      - On any mismatch, log
        `[EAGLE][Eagle3DraftLayer][fallback] ...`, reset the draft layer,
        but keep EAGLE enabled for non‑Eagle3 paths.

- [ ] **[B] Add `EagleModule::forward_draft_tree` using the new draft layer**
  - Replace the current “linearised” QKV+Wo draft head with a proper
    Eagle3 draft tree forward:
    - Input:
      - `last_hidden` [B, H] from target decode step,
      - `eagle_capture_hidden` [B, num_capture_layers * H_in] from
        `UnifiedDecoder`,
      - `EagleBuffers::inputs.draft_tokens`,
      - `EagleBuffers::inputs.draft_paths`,
      - tree masks/leaf masks and packed masks already wired in.
    - Steps (v1):
      - Compute `fc_out = captured_hidden @ eagle_fc.weight` [B, H].
      - For each tree node, build a single‑token draft sequence:
        - `hidden_in[n] = fc_out` (or a per‑node variant later),
        - `embeds[n]` from tok_embeddings for that node’s draft ID,
        - `position_ids[n]` consistent with target tree decode.
      - Flatten to `hidden_in_flat` / `embeds_flat` /
        `position_ids_flat` with `N = num_tree_tokens`.
      - Build `AttentionParams` for draft:
        - Q/K/V head layout matches target; for v1, KV can be
          “self‑only” (no prefix), i.e. `q_len = 1`, `k_len = 1`.
        - Later, extend to use prefix KV if needed for better parity.
      - Call `Eagle3DraftLayer::Forward` on this flattened batch to get
        `hidden_out_flat` [N, 1, H].
      - Apply LM head (shared or separate) to last timestep per node and
        write logits into `tree_logits_buffer_` or a dedicated draft
        logits buffer `[N, vocab_size_padded]`.
      - Derive per‑node draft IDs by reusing a device argmax helper
        (similar to `invokeTreeLogitsToTargetIds`) and write them into
        `EagleBuffers::inputs.draft_tokens` for acceptance.

- [ ] **[A/B] Integrate `forward_draft_tree` into `dynamicDecodeWithSpecMulti`**
  - In `LlamaV2::dynamicDecodeWithSpecMulti`’s EAGLE branch:
    - Remove the GPU Top‑K sampling over shallow `draft_logits` and the
      host logic that fabricates `draft_tokens` from those logits.
    - Instead call:
      ```cpp
      eagle_module_->forward_draft_tree(
          decoder_features,
          eagle_capture_hidden_,
          *eagle_buffers_,
          tree_logits_buffer_,
          stream_);
      ```
    - Leave target tree decode and `invokeTreeAcceptByIdsWithPaths`
      unchanged: only the source of draft IDs becomes the full Eagle3
      draft layer, not the approximate linear head.

- [ ] **[B/C] Expose Eagle3 draft intermediates via Python for HF/TRT comparison**
  - Extend the `_turbomind.eagle_forward_logits_debug` binding so that it
    runs the new Eagle3 draft **layer** on a single node and returns:
    - `fc_out` – FC output after Eagle3 capture+FC (3H→H),
    - `attn_out` – attention output before FFN,
    - `ffn_out` – GatedMLP output before final norm,
    - `pre_head_hidden` – post‑`output_norm` hidden,
    - `logits` – draft logits after LM head.
  - Use DLPack to expose these as CUDA tensors to Python.
  - Update `tests/turbomind/eagle3_compare.py` to:
    - Build HF Eagle3 equivalents (`fc_out_hf`, `attn_out_hf`,
      `ffn_out_hf`, `pre_head_hidden_hf`, `logits_hf`) using
      `Eagle3ForCausalLM` from TensorRT‑LLM.
    - Compute per‑stage metrics:
      - `mean_abs_diff`, `max_abs_diff`, cosine similarity for each of
        FC_OUT / ATTN_OUT / FFN_OUT / PRE_HEAD / LOGITS,
      - `argmax_match_rate` and top‑k overlap for logits.
    - Use these metrics as the primary loop for tuning Eagle3 geometry
      and residual/norm placement until overlap is clearly non‑zero.

- [ ] **[B] Iterate until Eagle3 draft logits overlap with HF/TRT and acceptance improves**
  - Use the extended `eagle3_compare.py` to drive numeric convergence:
    - For GPT‑OSS‑120B‑Eagle3, require:
      - `argmax_match_rate > 0` and non‑trivial top‑k overlap,
      - reasonably small per‑stage diffs (within expected tolerance for
        MXFP4/BF16 vs BF16).
  - Once draft logits are aligned:
    - Re‑run `inference/benchmark_speculative.py` / `run_spec_suite.sh`.
    - Expect to see:
      - `mean_acceptance_length > 1`,
      - `fraction_steps_accept_ge2 > 0`,
      - throughput ≥ baseline in key scenarios.
  - Only then consider turning on SpecPV partial KV for long‑context
    runs; SpecPV should not change acceptance, only reduce target tree
    decode cost at long contexts.

### 4.3 Final clean‑up for B/C (no scaffolding)

- [ ] **[B/C] Remove any remaining dead EAGLE draft paths**
  - After the Eagle3 draft layer is fully wired:
    - Delete the old “linearised” QKV+Wo draft head code paths that are
      no longer used for GPT‑OSS‑120B‑Eagle3.
    - Ensure there is exactly one active EAGLE3 draft implementation:
      the `Eagle3DraftLayer`‑based path driven from
      `dynamicDecodeWithSpecMulti` and `forward_draft_tree`.
    - Keep non‑Eagle3 (legacy EagleNet) draft paths intact and well
      guarded by `eagle_mode_`.

---

## 5. SpecPV‑Style Partial Verification for EAGLE3 (Next Phase)

These items describe a *new* optimisation layer on top of the fully
implemented EAGLE3 + target‑tree decode path. They are **not required**
for TensorRT‑LLM parity, but aim to bring SpecPV’s partial KV technique
into TurboMind for long‑context EAGLE3 runs.

### 5.1 Engine configuration and gating

- [x] **[A] Add SpecPV flags to EngineParam and Triton config**
  - Extend `EngineParam` (`llama_params.h`) with:
    - `bool enable_specpv{false};`
    - `int  specpv_block_size{16};`
    - `int  specpv_n_sink_blocks{2};`
    - `int  specpv_n_retrieval_blocks{256};`
    - `int  specpv_n_window_blocks{8};`
    - `int  specpv_n_spec_tokens_buf{128};`
    - `int  specpv_partial_threshold{4096};`
    - `int  specpv_full_refresh_steps{32};`
  - Parse these from Triton YAML / LMDeploy `SpeculativeConfig` in
    `LlamaTritonModel` / async_engine, mirroring the existing
    `enable_eagle_target_tree` wiring.

- [x] **[A] Add SpecPV gating helpers to LlamaV2**
  - Implement:
    - `bool isSpecPVEnabled() const;`
    - `bool shouldUseSpecPV(int seq_len) const;`
  - `isSpecPVEnabled()` should check `engine_param_.enable_specpv` and a
    `specpv_supported_` flag (e.g. TP/DP/PP constraints).
  - `shouldUseSpecPV(seq_len)` should mirror SpecPV’s `should_partial_verify`
    by requiring:
    - context length above `specpv_partial_threshold`,
    - a live partial KV cache object,
    - and enough KV budget for `eagleMaxEngineTokensPerStep() + 1` tokens.

### 5.2 PartialKVCache core (TurboMind analogue of SpecPV)

- [x] **[B] Implement SpecPVCacheConfig and PartialKVCache**
  - `SpecPVCacheConfig`:
    - Holds `block_size`, `n_sink_blocks`, `n_retrieval_blocks`,
      `n_window_blocks`, `n_spec_tokens_buf`.
    - Provides helpers:
      - `int sink_size() const;`
      - `int retrieval_size() const;`
      - `int window_size() const;`
      - `int total_budget() const;  // sink+retrieval+window+buffer`
  - `PartialKVCache`:
    - Owns per‑layer KV tensors: `[max_batch, num_kv_heads, total_budget, head_dim]`
      and slices `"sink"`, `"retrieval"`, `"window"`, `"buffer"` in token order.
    - Maintains per‑layer:
      - `summary_key_states` (Kmax/Kmin for blocks beyond sink),
      - `summary_block_count`,
      - `verified_lens[layer]`,
      - and a global `global_verified_lens`.
    - Methods:
      - `summary_key_states(layer_idx, key_states, seq_len)` – compute Kmax/Kmin
        for new blocks (Eq. (1) in SpecPV).
      - `refresh_retrieval(layer_idx, query_states, key_states, value_states, seq_len)` –
        block scoring and top‑K selection (Eq. (2),(3)); fill retrieval/window
        slices.
      - `update(layer_idx, new_keys, new_values)` – append newly verified tokens
        into the buffer slice and return active [sink+retrieval+window+buffer]
        KV views.
      - `init_from_full_kv(...)` – seed sink/retrieval/window from full KV once.
      - `reset()` / `reset_buffer()`.
  - Implementation status:
    - `SpecPVCacheConfig` and `PartialKVCache` are implemented in `specpv_kv_cache.{h,cc}` and wired into `LlamaV2` construction and SpecPV
      gating:
      - Per-layer KV buffers `[max_batch, num_kv_heads, total_budget, head_dim]` are allocated on device for both keys and values.
      - Segment views (`sink`, `retrieval`, `window`, `buffer`) are implemented for K and V.
      - `summary_key_states` computes real Kmax/Kmin block summaries (float32) on host; `refresh_retrieval` uses those summaries to select
        top blocks and fills retrieval/window K/V slices from full KV; `update` writes new K/V into the buffer and returns active
        `[sink+retrieval+window+buffer]` views; `reset_buffer` clears buffer K/V and resets verified lengths.
    - Seeding from the live TurboMind full-KV cache is now provided by `LlamaV2::initSpecPVFromFullKV` (Section 5.3), which flattens the
      prefix KV into `[B,H,L,D]` tensors (fp16/bf16 → float32) and calls `summary_key_states` / `refresh_retrieval` per layer. PartialKVCache
      and its CUDA helpers operate on float32 K/V internally, while the tree decode scratch KV is kept in fp16 or bf16 to match the base
      model KV dtype. Quantized KV caches (int8/int4) explicitly disable SpecPV and fall back to full-KV EAGLE3.

- [x] **[B] Attach PartialKVCache to LlamaV2**
  - Add a `SpecPVCacheConfig specpv_cache_config_` and
    `std::unique_ptr<PartialKVCache> specpv_kv_cache_` to `LlamaV2`.
  - Initialize them from `EngineParam` in the LlamaV2 constructor when
    `enable_specpv` is true and geometry is compatible.

### 5.3 SpecPV‑mode target‑tree decode

- [x] **[B] Add SpecPV mode to runEagleTargetTreeDecode**
  - Extend `LlamaV2::runEagleTargetTreeDecode` with a SpecPV branch:
    - Full‑KV mode (today’s behaviour) stays as the default when
      `!shouldUseSpecPV(seq_len)` or SpecPV is disabled.
    - SpecPV mode:
      - Uses `PartialKVCache` to construct the base KV for the target
        tree decode:
        - sink slice → earliest tokens (anchor),
        - retrieval slice → top‑scoring blocks,
        - window slice → most recent tokens,
        - buffer slice → space to append tree tokens’ KV.
      - Appends tree tokens’ K/V into the buffer via
        `PartialKVCache::update(...)`.
      - Runs `UnifiedDecoder::Forward` over this partial KV view instead
        of a full‑prefix scratch KV:
        - Build `h_prefix_blocks` / `h_extra_blocks` for the partial
          prefix and tree tokens only,
        - Construct `kv_block_ptrs` over SpecPV’s active K/V region
          (sink+retrieval+window+buffer_verified) plus tree scratch,
        - Set `h_k_len` for each slot to the partial prefix length plus
          `tree_len` instead of `prefix_len + tree_len` from full KV.
  - Keep tree masks (`spec_packed_mask`) and post‑decode logic
    (logits → target_tokens → acceptance) unchanged.
  - Implementation status:
    - `runEagleTargetTreeDecode` now has a SpecPV branch guarded by `isSpecPVEnabled()`, `specpv_retrieval_initialized_` and
      `shouldUseSpecPV(seq_len)`:
      - In SpecPV mode it computes a partial prefix length from the SpecPV budget and `PartialKVCache::global_verified_len()`, derives
        prefix block geometry from that, and allocates scratch blocks for all prefix+tree tokens.
      - Prefix K/V blocks for tree decode are populated from `PartialKVCache::active_prefix` into `scratch_kv_blocks` using the same block
        layout as the full-KV path; tree K/V and masks, logits→target_ids, and acceptance remain unchanged.
      - On any geometry/dtype/layout mismatch in this path, the code logs `[LlamaV2][SpecPV][fallback]`, disables SpecPV for the engine, and
        falls back to the full-KV EAGLE3 tree decode.

- [x] **[B] Seed SpecPV from a full‑KV tree step**
  - After the first full‑KV target‑tree decode at long context:
    - Call `LlamaV2::initSpecPVFromFullKV(committed_prefix_len)` to:
      - Flatten the full prefix KV cache per layer into contiguous
        `[B, H, L, D]` tensors (e.g. via an internal helper that reuses
        `invokeFlattenKV_v2`),
      - Call `PartialKVCache::summary_key_states` and
        `PartialKVCache::refresh_retrieval` for each layer to populate:
        - sink / retrieval / window K/V slices inside the partial cache,
      - Reset buffer usage (`reset_buffer`) and mark a
        `specpv_retrieval_initialized_` flag on `LlamaV2` so subsequent
        tree decodes can safely use the partial KV view.
  - Implementation status:
    - `LlamaV2::initSpecPVFromFullKV` now flattens the full-prefix KV cache per layer into `[B,H,L,D]` tensors via
      `flattenPrefixKVForLayer` (which reuses `invokeFlattenKV_v2`), then calls `PartialKVCache::summary_key_states` and
      `PartialKVCache::refresh_retrieval` to populate sink/retrieval/window slices. The speculative buffer is reset so verified prefix
      tokens are represented only via these slices.
    - `updateSpecPVAfterAcceptance` drives seeding and refresh after DynamicDecode+EAGLE acceptance: it computes `max_len` from committed
      `sequence_length`, calls `initSpecPVFromFullKV(max_len, ...)` when `shouldUseSpecPV(max_len)` is true, and uses
      `specpv_partial_steps_` plus buffer headroom to trigger a full-refresh reset (dropping back to full-KV tree decode for the next
      step and reseeding SpecPV from the new full prefix).

### 5.4 Partial/full switching and refresh

- [x] **[A] Implement SpecPV partial/full switching policy**
  - Inside `LlamaBatch::Forward` / `LlamaV2`:
    - Use `shouldUseSpecPV(seq_len)` to decide:
      - Full‑KV tree decode (default) vs SpecPV mode for this step.
    - Track:
      - A per‑engine partial‑step counter.
      - A per‑engine “buffer usage” indicator from `PartialKVCache`:
        `verified_lens[layer]` vs `n_spec_tokens_buf`.
  - Force full‑KV tree decode and partial cache refresh when:
    - The partial‑step counter exceeds `specpv_full_refresh_steps`, or
    - Buffer usage approaches `n_spec_tokens_buf`, or
    - Any KV invariant or geometry check fails for the SpecPV path.
  - Implementation status:
    - `LlamaV2::shouldUseSpecPV(seq_len)` gates SpecPV based on context length and partial KV budget; `updateSpecPVAfterAcceptance`
      increments `specpv_partial_steps_` when partial mode is active and computes a buffer-close-to-full condition from
      `PartialKVCache::global_verified_len()` and `SpecPVCacheConfig::buffer_size()`.
    - When either threshold is exceeded, `updateSpecPVAfterAcceptance` logs a full-refresh trigger, calls `specpv_kv_cache_->reset()`,
      clears `specpv_retrieval_initialized_` and `specpv_partial_steps_`, ensuring the next step uses the full-KV tree decode path before
      reseeding SpecPV from the refreshed prefix.

- [ ] **[B] Update PartialKVCache after EAGLE acceptance**
  - After multi‑token acceptance and commit:
    - Use the committed EAGLE extras to:
      - Update `partial_kv_cache_->update(...)` per layer with the
        newly verified tokens (keys/values from TurboMind KV).
      - Increment `verified_lens[layer]` accordingly.
    - Keep `global_verified_lens` aligned with the full‑KV prefix as in
      SpecPV:
      - Only bump `global_verified_lens` on full‑KV refresh steps.
  - Implementation status:
    - V1 is now **incremental**:
      - `LlamaV2::updateSpecPVAfterAcceptance` seeds SpecPV from the live full‑KV prefix via `initSpecPVFromFullKV` the first time (or
        after a full‑refresh), and on subsequent steps uses `flattenPrefixKVForLayer` + `PartialKVCache::update(...)` to append newly
        committed tail tokens into the SpecPV buffer per layer.
      - `PartialKVCache::verified_lens_` / `global_verified_len_` track the number of buffered tail tokens, and `runEagleTargetTreeDecode`
        uses this to bound the effective history length seen by tree decode in SpecPV mode.
    - Full‑refresh policy remains in place: when `specpv_partial_steps_` or buffer headroom thresholds are exceeded, SpecPV is reset and
      the next step runs full‑KV tree decode before reseeding the partial cache from the new prefix.

### 5.5 Interaction with existing KV and EAGLE paths

- [x] **[C] Keep SpecPV orthogonal to EAGLE semantics**
  - Ensure SpecPV does not change:
    - EAGLE3 draft head maths or geometry.
    - Tree masks / structure.
    - `invokeTreeAcceptByIdsWithPaths` semantics (ID‑equality).
  - SpecPV should only affect:
    - Which KV entries the target attends to during tree decode.
    - How KV is updated and reused between partial/full verification
      steps.
  - Implementation status:
    - SpecPV only changes which KV entries the target attends to during target-tree decode (by replacing prefix K/V with the partial
      cache in `runEagleTargetTreeDecode`). Tree structure, masks, EAGLE draft/acceptance kernels, and EOS/stop/max_new_tokens semantics
      remain unchanged.

- [x] **[C] Maintain baseline and TRT‑LLM parity modes**
  - When `enable_specpv == false`:
    - Behaviour must remain identical to the now‑completed EAGLE3 /
      target‑tree implementation (Sections 1–4).
  - SpecPV mode should be easy to toggle at runtime via config so we can
    benchmark:
    - Full‑KV TurboMind EAGLE3 vs SpecPV‑enabled TurboMind EAGLE3.
  - Implementation status:
    - When `enable_specpv == false` or SpecPV fails any invariant, `isSpecPVEnabled()` is false and the engine follows the completed
      full-KV EAGLE3 target-tree implementation (Sections 1–4). When enabled and supported, SpecPV is confined to the target-tree decode
      KV layout, and can be toggled per-engine via `EngineParam.enable_specpv` and related fields without affecting non-EAGLE paths.


---

## 5. Advanced: SpecPV‑Style Partial Verification on Top of EAGLE3 Target‑Tree Decode

This section sketches **future work**: integrating a SpecPV‑style *partial KV verification* path under the existing EAGLE3 target‑tree decode in TurboMind, to reduce verification cost for very long contexts. It is not required for TensorRT‑LLM parity, but it becomes relevant once 32K–64K contexts are common.

Reference implementation and paper:
- Code: `SpecPV/specpv/kv/kv_cache.py`, `specpv/speculate/{speculator.py,utils.py,configs.py}`, `specpv/models/modeling_llama_kv.py`.
- Paper: `SpecPV/PAPER.md`, especially §3.2 *Partial Verification*, §3.3 *Rectified with Full Verification*, Fig. 2, and Algorithm 1.

### 5.1 Conceptual overview

- **Problem:** At long context, even with EAGLE3 draft + target‑tree decode, verification dominates latency because attention always sees the full prefix KV.
- **SpecPV idea:** Maintain a *partial* target KV cache for verification consisting of:
  - **Sink tokens:** initial KV blocks kept always.
  - **Retrieval tokens:** blocks selected via block‑level summaries (`Kmax`, `Kmin`) and query similarity scores.
  - **Local window:** a fixed‑size tail of the most recent tokens.
  - **Spec buffer:** candidate / partially verified tokens awaiting correction.
- **Behaviour:**
  - For short context, verification uses full KV (no partial cache).
  - When context exceeds a partial KV budget:
    - Run one **full verification** step to initialize the partial KV.
    - Then run most steps in **partial verification** mode (tree decode attends only to partial KV + buffer).
    - Periodically perform full verification again to refresh the partial KV and clear accumulated errors.

### 5.2 Proposed engine‑level API and config

These are additive fields on top of existing speculative / EAGLE config; when disabled, behaviour is identical to today.

- Extend `EngineParam` (`src/turbomind/models/llama/llama_params.h`):
  - `bool enable_specpv{false};`
  - `int  specpv_block_size{16};`
  - `int  specpv_n_sink_blocks{2};`
  - `int  specpv_n_retrieval_blocks{256};`
  - `int  specpv_n_window_blocks{8};`
  - `int  specpv_n_spec_tokens_buf{128};`
  - `int  specpv_partial_threshold{4096};`  // context length above which partial KV can kick in
  - `int  specpv_full_refresh_steps{32};`   // max partial steps before a forced full verify

- Wire through Triton YAML `speculative_config` (alongside `enable_target_tree`), and through LMDeploy’s Python `SpeculativeConfig` object so users can enable SpecPV for EAGLE3 targets explicitly.

### 5.3 PartialKVCache data structure (C++ analogue of SpecPV)

Introduce a TurboMind‑side partial KV cache that mirrors SpecPV’s `PartialKVCache`:

- New `SpecPVCacheConfig`:
  - Fields: `block_size`, `n_sink_blocks`, `n_retrieval_blocks`, `n_window_blocks`, `n_spec_tokens_buf`.
  - Derived: `sink_size`, `retrieval_size`, `window_size`, `total_budget`.

- New `PartialKVCache` (C++ class, files `specpv_kv_cache.{h,cc}`):
  - For each layer:
    - Allocate `[max_batch_size, num_kv_heads, total_budget, head_dim]`.
    - Slice into:
      - `"sink"`   : first `sink_size` tokens,
      - `"retrieval"`: next `retrieval_size` tokens,
      - `"window"`: next `window_size` tokens,
      - `"buffer"`: final `n_spec_tokens_buf` tokens.
  - Maintain:
    - `key_states_summary[layer]["max"/"min"]` for block summaries, shape `[B, H, max_blocks, D]`.
    - `summary_block_count[layer]` (how many blocks have summaries).
    - `verified_lens[layer]` (how many tokens in buffer are now fully verified).
    - `global_verified_lens` (length of the fully verified prefix over which retrieval is defined).
  - Methods (1:1 with SpecPV):
    - `summary_key_states(layer_idx, key_states, seq_len)`:
      - Implements Eq. (1) of the paper (per‑block `Kmax`, `Kmin`).
    - `refresh_retrieval(layer_idx, query_states, key_states, value_states, seq_len)`:
      - Implements Eq. (2),(3) and `refresh_retrieval` in `kv_cache.py`:
        - Compute similarity scores vs summaries for each block,
        - Select top‑`n_retrieval_blocks`,
        - Gather keys/values into `"retrieval"` slice,
        - Fill `"window"` with last `window_size` tokens.
    - `update(layer_idx, new_key_states, new_value_states)`:
      - Append KV for newly (partially) verified tokens into `"buffer"` and return the active `[sink+retrieval+window+buffer]` view.
    - `init_key_values(full_kv)`:
      - Seed `sink` (and optionally initial retrieval/window) from full KV once, like SpecPV’s `init_key_values`.
    - `get_seq_length(layer_idx)`, `reset()`, `reset_buffer()`.

`LlamaV2` should hold:
- `std::unique_ptr<PartialKVCache> specpv_kv_cache_;`
- `SpecPVCacheConfig specpv_cache_config_;`
- Flags: `bool specpv_supported_{false};`.

### 5.4 Gating and integration with `runEagleTargetTreeDecode`

With the data structure in place, EAGLE3 tree decode can choose between full vs partial KV:

- New helpers on `LlamaV2`:
  - `bool isSpecPVEnabled() const`:
    - `return engine_param_.enable_specpv && specpv_supported_ && specpv_kv_cache_ != nullptr;`
  - `bool shouldUseSpecPV(int sequence_len) const`:
    - Mirrors SpecPV’s `should_partial_verify`:
      - `isSpecPVEnabled()`,
      - `sequence_len > specpv_partial_threshold`,
      - `specpv_kv_cache_->enabled && specpv_kv_cache_->retrieval_initialized`,
      - `specpv_kv_cache_->get_seq_length() + eagle_max_engine_tokens_per_step_ + 1 <= specpv_cache_config_.total_budget()`.

- In `LlamaV2::runEagleTargetTreeDecode`:
  - Full‑KV mode (**current behaviour**):
    - When `!shouldUseSpecPV(sequence_len)`:
      - Build scratch KV from full prefix blocks and run `UnifiedDecoder::Forward` as today.
  - SpecPV mode (**partial KV**):
    - When `shouldUseSpecPV(sequence_len)`:
      - Use `specpv_kv_cache_` as the base KV view:
        - `sink` + `retrieval` + `window` slices define the “trusted” prefix.
      - Append KV for tree tokens into `"buffer"` via `PartialKVCache::update`.
      - Run `UnifiedDecoder::Forward` with that composite KV instead of full prefix.

Tree masks, tree logits → `target_tokens`, and acceptance remain unchanged: SpecPV only narrows which KV positions the target attends to during tree decode.

### 5.5 Maintaining `global_verified_lens` / `verified_lens` and periodic full refresh

The bookkeeping should follow SpecPV’s `global_verified_lens` and per‑layer `verified_lens` semantics:

- **Full‑KV steps (initialization or refresh):**
  - After a tree decode step that used full KV:
    - Set `specpv_kv_cache_->global_verified_lens = current_sequence_len` for each active sequence.
    - Call `initSpecPVFromFullKV` (helper on `LlamaV2`) to seed `sink` / `retrieval` / `window` from the live full KV.
    - Reset `verified_lens[layer]` and buffer.

- **Partial‑KV steps:**
  - After `invokeTreeAcceptByIdsWithPaths` and `advanceSequencesByEagleAcceptance`:
    - Determine how many **extra tokens** were actually committed for each sequence.
    - Append their KV into the partial `"buffer"` for each layer via `PartialKVCache::update`.
    - Increment `verified_lens[layer]` accordingly.
    - Leave `global_verified_lens` unchanged (it still points to the fully verified prefix before partial steps), mirroring SpecPV’s `tree_decoding` + `update_inference_inputs` separation.

- **Periodic full refresh:**
  - When any of the following holds:
    - `verified_lens[layer]` approaches `n_spec_tokens_buf` (buffer nearly full),
    - Number of consecutive partial steps reaches `specpv_full_refresh_steps`,
    - Or the effective `get_seq_length()` is too close to `total_budget`.
  - Run a full‑KV `runEagleTargetTreeDecode` step, then:
    - Rebuild partial KV from full KV (`initSpecPVFromFullKV`),
    - Reset buffer and `verified_lens` counters.

At all times:
- If any SpecPV invariant fails (dtype/layout mismatch, buffer too small for `eagle_max_engine_tokens_per_step_`, unexpected KV geometry), log a `[LlamaV2][SpecPV][fallback] ...` message, set `specpv_supported_ = false`, free `specpv_kv_cache_`, and continue with the current full‑KV EAGLE3 behaviour.
