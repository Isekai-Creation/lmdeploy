# TurboMind EAGLE‑3 – Design & TODO Checklist

This file is the **implementation + design TODO** for bringing TurboMind’s
EAGLE‑3 path to **production‑grade parity** with TensorRT‑LLM on
**GPT‑OSS‑120B‑Eagle3**.

Status markers:
- `[x]` implemented in this tree (may still need tuning)
- `[ ]` not implemented / still to do

Roles (for reference):
- **A – Kernels & math** (attention, RoPE, KV layout, CUDA)
- **B – Model & converter** (HF → draft dir, geometry, config)
- **C – TurboMind integration** (LlamaV2, UnifiedDecoder, KV, LMDeploy)
- **D – Validation & infra** (tests, debug harnesses, benchmarks)

---

## 0. Scope & Success Criteria

**Functional parity (vs TensorRT‑LLM Eagle‑3 on GPT‑OSS‑120B)**
- [ ] Multi‑token EAGLE‑3 draft tree decode (static tree for GPT‑OSS‑120B).
- [ ] **Real Eagle‑3 attention** in the draft head
      (no “shallow QKV hack” in normal configs).
- [ ] Correct acceptance, tail commit, and KV reuse under:
      - dynamic batching,
      - block reuse / block manager semantics,
      - mixed prefill + decode (offline pipeline).

**Numeric parity**
- [ ] Stage‑wise draft numerics for GPT‑OSS‑120B‑Eagle3:
      - ATTN_OUT / FFN_OUT / PRE_HEAD / LOGITS:
        TM vs HF/TRT within agreed tolerances (cosine, max diff).
- [ ] Speculative acceptance behaviour consistent with TRT‑LLM
      for the **same model + prompts** (distribution of accepted lengths,
      EOS/stop behaviour).

**Performance parity**
- [ ] Mean acceptance length in the **4–7 tokens** range on
      GPT‑OSS‑120B‑Eagle3 workloads comparable to TRT‑LLM.
- [ ] Throughput improvement vs baseline TurboMind (no spec) is
      **≥ TRT‑LLM Eagle‑3** on the same hardware class (H100/B200/GB200)
      for representative prompts (8K/32K context).

**Integration**
- [x] Works in **offline pipeline** (TurboMind engine via LMDeploy).
- [ ] Coexists cleanly with:
      - non‑speculative decode,
      - other speculative methods (vanilla, lookahead, Medusa‑style),
      - **SpecPV** (partial KV) as an optional long‑context optimisation.

---

## 1. Phase 0 – Spec Extraction & Design Alignment (TRT‑LLM + EAGLE)

Goal: a short **design doc** that is the source of truth for the rest of the
work. No further refactors until this is written and reviewed.

**Where to read (TRT‑LLM side):**
- `tensorrt_llm/models/eagle/model.py`
- `tensorrt_llm/_torch/speculative/eagle3.py`
- `tensorrt_llm/_torch/speculative/drafting_loops.py`
- `tensorrt_llm/_torch/models/modeling_speculative.py`
  (Eagle3Attention / Eagle3DecoderLayer / Eagle3DraftModel / Eagle3ForCausalLM)
- KV + attention meta:
  - `tensorrt_llm/_torch/attention_backend/trtllm.py`
  - `tensorrt_llm/_torch/metadata.py` (`KVCacheParams`, `AttentionMetadata`)
- E2E & Eagle‑3 tests:
  - `tests/unittest/_torch/speculative/test_eagle3.py`
  - `tests/unittest/_torch/speculative/test_kv_cache_reuse.py`
- Docs:
  - `docs/source/blogs/tech_blog/blog11_GPT_OSS_Eagle3.md`
  - `docs/source/advanced/speculative/eagle3_*` (if present)

**Where to read (EAGLE repo side):**
- `EAGLE/eagle/modeling_eagle.py`
- EAGLE‑3 training/inference scripts:
  - `EAGLE/eagle/traineagle3/*`
  - `EAGLE/eagle/application/*`

**Deliverables (doc to live under `docs/` – name TBC):**

- **Design doc**: see `docs/turbomind_eagle3_trtllm_design.md` for the
  current reference implementation of 1.1–1.3.

- [x] **1.1 Eagle‑3 inference loop (TRT‑LLM) – GPT‑OSS‑120B**
  - Diagram the flow for *one step* of speculative decoding:
    - Base model forward (GPT‑OSS‑120B) with KV reuse.
    - Hidden capture for Eagle‑3 (which layers; how concatenated).
    - Eagle‑3 FC + attention + MLP + LM head → draft logits.
    - Tree structure construction (static tree for Eagle‑3).
    - Acceptance and KV cache updates.
  - Include:
    - `Eagle3ResourceManager.hidden_states` shape:
      `[max_tokens, hidden_size * num_capture_layers]`.
    - How `layers_to_capture` translates into the FC input layout.
    - How `max_draft_len` and `max_total_draft_tokens` feed into KV
      over‑provisioning (extra KV tokens).

- [x] **1.2 Tensor shapes at each Eagle‑3 draft stage**
  - For GPT‑OSS‑120B‑Eagle3 specifically:
    - Pre‑FC input: `[B, hidden * num_capture_layers]` or equivalent.
    - `fc.weight` from Eagle‑3 draft: `[hidden, hidden * N]` (EAGLE code +
      HF checkpoint; N≈3 for current GPT‑OSS checkpoint).
    - q/k/v/o projections:
      - HF/EAGLE: `midlayer.self_attn.{q,k,v,o}_proj.weight`.
      - TRT‑LLM: shapes used in attention kernels.
    - Attention outputs and LM head input/output:
      - context vector dimension,
      - LM head weight layout (row/column major).

- [x] **1.3 Mapping TRT‑LLM ↔ TurboMind components**
  - TRT‑LLM:
    - Eagle‑3 PyTorch stack:
      - `Eagle3SpecMetadata`, `Eagle3ResourceManager`,
      - Eagle‑3 **one‑model** worker / sampler,
      - drafting loops & KV reuse.
    - Engine pieces:
      - draft QKV + attention kernels,
      - KV layout (paged KV, extra KV tokens per sequence).
  - TurboMind:
    - `EagleModule` / `Eagle3DraftLayerWeight` / `Eagle3DraftLayer`.
    - `UnifiedDecoder::ForwardDraft`.
    - `LlamaV2::dynamicDecodeWithSpecMulti` /
      `runEagle3DraftTreeDecode` / `runEagleTargetTreeDecode`.
    - `SequenceManager`, KV layout, block manager.
  - The doc should state *explicitly* which TRT‑LLM behaviours we are
    reproducing and which we intentionally approximate (e.g. if we keep
    single‑position draft attention initially).

---

## 2. Phase 1 – Converter & Weight Layout (EAGLE‑3 Draft)  – **Owner: B**

Current state:
- **[x]** `lmdeploy/turbomind/eagle_draft_converter.py`:
  - Detects `eagle3_midlayer` layout by inspecting HF keys.
  - Writes `config.yaml` with:
    - `hidden_units`, `vocab_size`, `head_num`, `size_per_head`,
      `inter_size`,
    - Eagle‑3 metadata: `eagle_q_size`, `eagle_kv_size`,
      `eagle_qkv_in_dim`, `eagle_fc_in_dim`, `eagle_mode="eagle3"`.
  - `_convert_eagle3_midlayer`:
    - Writes norms, MLP weights from `midlayer.*`.
    - Validates `fc.weight` shape `(hidden, 3 * hidden)`.
    - Exports:
      - `fc.weight` (legacy shallow path),
      - `eagle_fc.weight` (`[3*hidden, hidden]`).
    - Builds real attention weights **from the base model’s** layer‑0
      Q/K/V/O into TurboMind’s LlamaAttention layout:
      - `layers.0.attention.w_qkv.weight`: `[hidden, 3 * hidden]`.
      - `layers.0.attention.wo.weight`:  `[attn_hidden, hidden]` (draft).

**TODOs:**

- [ ] **2.1 Lock in Eagle‑3 geometry for GPT‑OSS‑120B**
  - Using HF configs:
    - `openai/gpt-oss-120b`,
    - `nvidia/gpt-oss-120b-Eagle3`,
    - Eagle repo’s `EAGLEConfig` in `modeling_eagle.py`.
  - Document in the design doc:
    - `hidden_size`, `intermediate_size`,
    - `num_attention_heads`, `num_key_value_heads`, head dim,
    - Eagle‑3 FC fan‑in factor (`eagle_fc_in_factor`) and
      Q/K/V sizes used in the draft head.

- [x] **2.2 Reconcile converter layout with TRT‑LLM Eagle‑3**
  - Current converter behaviour (`lmdeploy/turbomind/eagle_draft_converter.py`):
    - detects `eagle3_midlayer` layout and writes:
      - `eagle_fc.weight` (`[3*hidden, hidden]`) from `fc.weight`,
      - native Eagle‑3 midlayer projections to:
        - `eagle3.q_proj.weight`,
        - `eagle3.k_proj.weight`,
        - `eagle3.v_proj.weight`,
        - `eagle3.o_proj.weight`,
        enforcing strict shape checks against the geometry inferred by
        `_infer_eagle3_geometry`.
    - when `base_model_dir` is provided, additionally exports a
      LLaMA‑layout attention backbone from the target model’s
      `model.layers.0.self_attn.{q,k,v,o}_proj.weight` into:
      - `layers.0.attention.w_qkv.weight`,
      - `layers.0.attention.wo.weight`,
      for use by the shallow draft path.
    - records Eagle‑3 geometry (`eagle_q_size`, `eagle_kv_size`,
      `eagle_qkv_in_dim`, `eagle_fc_in_dim`, and factors) in
      `config.yaml`, which `EagleModule::load` consumes to allocate
      `EagleWeight` and initialise `Eagle3AttentionWeight`.

- [ ] **2.3 Strengthen config + shape validation**
  - Add explicit unit tests under `tests/turbomind` to:
    - run `prepare_eagle_draft_from_hf` on a small Eagle‑3 checkpoint
      (or a synthetic subset),
    - read back `config.yaml` and all `.weight` files,
    - assert shapes/dtypes:
      - `layers.0.attention.w_qkv.weight`,
      - `layers.0.attention.wo.weight`,
      - `fc.weight`, `eagle_fc.weight`,
      - MLP and norm weights.
  - Converter should:
    - **fail fast** with clear messages if the HF checkpoint deviates
      from the expected Eagle‑3 layout (rather than silently producing
      an approximate draft).

---

## 3. Phase 2 – Eagle‑3 Draft Attention & Capture  – **Owner: A**

Current state:
- **[x]** `Eagle3DraftLayerWeight` aggregates:
  - `LlamaAttentionWeight` + `LlamaFfnWeight` + norm tensors.
- **[x]** `Eagle3DraftLayer::Forward`:
  - `input_norm` RMSNorm → attention backend → `post_attn_norm` RMSNorm →
    FFN → residual + `output_norm` RMSNorm.
  - Today the attention backend is:
    - `UnifiedAttentionLayer` when geometry matches standard Llama
      (Q/K/V/O all `hidden × hidden`, fused QKV `[hidden, 3*hidden]`),
    - **fallback** shallow QKV+V→Wo path otherwise.
  - This is **not compatible** with GPT‑OSS‑120B‑Eagle3 midlayer shapes:
    - `fc.weight`         = `[hidden=2880, 3*hidden=8640]`
    - `q_proj.weight`     = `[4096, 2880]`
    - `k_proj/v_proj`     = `[512, 2880]`
    - `o_proj.weight`     = `[2880, 4096]`
    - i.e. non‑square, non‑uniform Q/K/V dims that cannot be “reshaped”
      into a standard Llama attention layout without changing the math.
  - Robust shape/dtype checks are already in place and correctly refuse
    to “magically” reinterpret these weights.
- **[x]** `UnifiedDecoder::ForwardDraft` calls `Eagle3DraftLayer` when
          present; draft logits are produced via `LlamaLinear` LM head.

Known gap:
- **[ ]** There is **no dedicated Eagle‑3 attention backend** today:
  - GPT‑OSS‑120B‑Eagle3 midlayer geometry cannot be expressed via
    `UnifiedAttentionLayer` without rewriting kernels.
  - Any attempt to reshape `[4096×2880, 512×2880, 2880×4096]` weights
    into a `[hidden, 3*hidden]` Llama layout would produce numerically
    wrong and unstable behaviour, and would not match TRT‑LLM.
  - Draft attention also currently runs with **q_len = kv_len = 1**
    (single‑position semantics) and does **not** reuse KV along the
    tree like TRT‑LLM Eagle‑3.

**TODOs:**

- [ ] **3.1 Introduce a dedicated Eagle‑3 attention backend**
  - Add an `Eagle3AttentionWeight` struct that holds:
    - Q: `[q_dim, hidden]`   (e.g. 4096×2880),
    - K/V: `[kv_dim, hidden]` (e.g. 512×2880),
    - O: `[hidden, q_dim]`   (e.g. 2880×4096),
    - any head metadata needed to map these into heads for CUDA kernels.
  - Implement an `Eagle3AttentionLayer` with its own CUDA kernels that:
    - consumes FC‑projected Eagle‑3 input (e.g. `[B, hidden]` or `[B, L, hidden]`),
    - applies Q/K/V projections with the Eagle‑3 geometry,
    - applies RoPE and softmax exactly as in EAGLE3/TRT‑LLM,
    - produces context vectors in the expected dimension (e.g. q_dim),
      followed by `o_proj` back into `hidden`.
  - Plug `Eagle3AttentionLayer` into `Eagle3DraftLayer::Forward` when
    `eagle_mode == "eagle3"` and the weight shapes match GPT‑OSS‑120B,
    instead of going through `UnifiedAttentionLayer`.
  - Keep strict shape checks; on mismatch:
    - log a clear `[EAGLE3][Attention][fallback]` warning,
    - fall back to a pass‑through draft (no speculative gains), but do
      **not** attempt to reinterpret weights.

- [ ] **3.2 Guarantee real Eagle‑3 attention in normal Eagle‑3 runs**
  - For GPT‑OSS‑120B‑Eagle3 (and other supported Eagle‑3 drafts):
    - ensure `EagleModule::load` builds `Eagle3AttentionWeight` from
      `midlayer.self_attn.{q,k,v,o}_proj.weight` and exposes it to
      `Eagle3DraftLayer`,
    - ensure `Eagle3DraftLayer::Forward` always uses
      `Eagle3AttentionLayer` (and never the shallow QKV path) under
      valid configs.
  - Add a debug counter / log (gated by `eagle_debug`) that reports,
    per run:
    - how often Eagle‑3 attention is used,
    - how often we fall back to pass‑through or legacy paths.

- [ ] **3.3 Align draft FC + capture layout with EAGLE/TRT‑LLM**
  - From EAGLE + TRT‑LLM:
    - Confirm which layers’ hidden states are concatenated into the
      Eagle‑3 FC input (e.g. `[low, mid, high]` layer selection).
    - Confirm the FC input ordering and any norms applied before FC.
  - In TurboMind:
    - Verify that `eagle_capture_hidden_` and `eagle_fc_in_dim_`
      correspond to the same capture pattern.
    - If necessary, adjust the capture ordering / concatenation in
      `UnifiedDecoder` and `EagleModule::load` so FC sees the same
      feature arrangement as the Eagle‑3 training setup.

- [ ] **3.4 (Optional for strict TRT parity) multi‑position draft attention**
  - If TRT‑LLM uses **q_len>1** in the draft tree for Eagle‑3:
    - Add a `ForwardTree` or extended `Forward` that:
      - accepts `[B, q_len, hidden]`,
      - reuses KV for draft tokens exactly as the main decoder does.
    - Wire this from `runEagle3DraftTreeDecode` so draft tokens per
      layer are produced using the same attention geometry as TRT‑LLM.
  - If TRT‑LLM keeps q_len=1 for Eagle‑3, document that and mark this
    item as “not required”.

- [ ] **3.5 Clean up shallow draft path once numerics are good**
  - Once 4.x validation (below) shows Eagle‑3 numerics are solid:
    - Restrict the shallow QKV path to:
      - explicit debug builds, or
      - non‑Eagle‑3 legacy modes.
    - Ensure `EagleModule` does **not** participate in Eagle‑3 drafting
      at all (only UnifiedDecoder + Eagle‑3 draft layer).

---

## 4. Phase 3 – TurboMind Integration & KV Semantics  – **Owner: C**

Current state (TurboMind side):
- **[x]** `LlamaV2::dynamicDecodeWithSpecMulti`:
  - For `spec_method == "eagle3"`:
    - calls `runEagle3DraftTreeDecode` to generate draft tokens +
      initial target tokens,
    - builds a tree via `eagle::SpeculationTree` using default choices
      from `EagleModule::getDefaultChoices()`,
    - mirrors paths into `EagleBuffers::inputs.draft_paths`,
    - builds leaf + packed masks (tree attention + acceptance),
    - uses device‑side acceptance (`invokeTreeAcceptByIdsWithPaths`,
      `invokePackAcceptedPaths`),
    - commits extra tokens via `DynamicDecodeLayer::ForwardMultiStep`
      + KV rewind driven by committed tails.
- **[x]** `targetTreeDecode` + `runEagleTargetTreeDecode`:
  - prepare tree inputs (`eagle_net_*` tensors),
  - compact tree tokens, embed them, and run a **scratch‑KV** target
    decode pass that reuses prefix KV read‑only,
  - write per‑node `target_tokens` back into `EagleBuffers`.
- **[x]** KV invariants:
  - Tree decode never mutates live `SequenceManager` KV:
    - prefix blocks reused as read‑only,
    - tree tokens live in scratch KV and are discarded,
    - only `KVCacheRewind` mutates KV after acceptance.
- **[x]** SpecPV path:
  - partial KV cache (`PartialKVCache`) can be used instead of full
    prefix KV for target‑tree decode, with clear fallbacks on mismatch.

**TODOs:**

- [ ] **4.1 Formalise target‑tree + acceptance contract vs TRT‑LLM**
  - In the design doc, explicitly compare:
    - TurboMind’s `eagle::SpeculationTree` default choices +
      packed masks,
    - TRT‑LLM’s Eagle‑3 tree semantics:
      - static vs dynamic tree,
      - branching factor and depth for GPT‑OSS‑120B,
      - acceptance algorithm (ID equality, root token behaviour).
  - Confirm that:
    - TurboMind’s `invokeTreeAcceptByIdsWithPaths` and
      `invokePackAcceptedPaths` implement the same semantics as
      TRT‑LLM for the static tree used in GPT‑OSS‑120B‑Eagle3.

- [ ] **4.2 Dynamic batching, block reuse, and KV invariants**
  - Re‑run and extend KV‑related tests with EAGLE‑3 enabled:
    - existing TurboMind KV tests,
    - TRT‑LLM tests like `test_kv_lens_runtime_with_eagle3_one_model`
      as inspiration (num_extra_kv_tokens semantics).
  - For TurboMind:
    - assert that `Sequence::blocks` and `cache_len` are consistent
      after many speculative steps with variable batch sizes.
    - ensure KV over‑provisioning for EAGLE matches what TRT‑LLM does
      for `max_draft_len` / `max_total_draft_tokens` (conceptually).

- [ ] **4.3 LMDeploy integration surface**
  - Ensure LMDeploy users can configure Eagle‑3 in a way that mirrors
    TRT‑LLM’s `EagleDecodingConfig`:
    - `method="eagle3"`, `num_speculative_tokens`, `max_draft_len`,
      `eagle_choices` (optional override), `use_dynamic_tree` (if
      supported), `max_total_draft_tokens`.
  - Validate that:
    - offline `pipeline(...)` and `TurbomindEngineConfig` accept
      Eagle‑3 config cleanly,
    - misconfigurations fail fast with clear messages.

---

## 5. Phase 4 – Numeric & Behavioural Validation  – **Owner: D**

Goal: prove TurboMind+EAGLE‑3 behaves like TRT‑LLM Eagle‑3 on
GPT‑OSS‑120B, within agreed tolerances.

**5.1 Stage‑wise draft compare (HF/TRT vs TurboMind)**
- [ ] Extend `tests/turbomind/eagle3_compare.py` to:
  - call a debug binding (`_turbomind.eagle3_forward_debug`) that:
    - runs `Eagle3DraftLayer::Forward` + LM head,
    - exposes intermediate tensors:
      - `fc_out`, `attn_out`, `ffn_out`, `pre_head_hidden`, `logits`.
  - For each stage available from HF/TRT and TurboMind:
    - compute `mean_abs_diff`, `max_abs_diff`, cosine similarity.
  - For `logits` specifically:
    - compute argmax match rate and top‑k overlap (e.g. top‑5).
- [ ] Define tolerances (documented in the design doc), e.g.:
  - cosine(logits) > 0.98,
  - top‑5 overlap > 0.8,
  - max_abs_diff within expected BF16 / MXFP ranges.

**5.2 Speculative acceptance metrics vs TRT‑LLM**
- [ ] Use `SpeculativeDecodingStats` / `EagleMetricsSummary` and
      TRT‑LLM’s test patterns to compare:
  - `mean_acceptance_length`,
  - `fraction_steps_accept_ge2`,
  - `eagle_total_draft_tokens`, `eagle_total_accepted_tokens`.
- [ ] On GPT‑OSS‑120B‑Eagle3:
  - run the same prompt sets in:
    - TRT‑LLM (per blog11),
    - TurboMind + LMDeploy,
  - compare acceptance distributions; investigate discrepancies beyond
    an agreed threshold.

**5.3 Throughput benchmarks**
- [ ] Extend `inference/benchmark_speculative.py` / `run_spec_suite.sh`
      with a GPT‑OSS‑120B‑Eagle3 profile:
  - baseline (no spec),
  - Eagle‑3 (TurboMind),
  - optionally TRT‑LLM for reference.
- [ ] Target:
  - TurboMind Eagle‑3 ≥ TurboMind baseline throughput,
  - TurboMind Eagle‑3 ≈ TRT‑LLM Eagle‑3 on the same hardware class in
    key scenarios (8K/32K contexts, realistic prompts).

**5.4 Regression & CI**
- [ ] Add a minimal CI spec test:
  - small Eagle‑3 draft (or synthetic),
  - one short prompt,
  - asserts:
    - no EAGLE fallback warnings,
    - acceptance is neither 0 nor “always full accept”,
    - token stream is stable against a recorded golden log
      (within allowed stochasticity).

---

## 6. Phase 5 – SpecPV (Partial KV) Follow‑Ups  – **Owner: A/C**

Current state:
- **[x]** `SpecPVCacheConfig` + `PartialKVCache`:
  - segments (sink, retrieval, window, buffer),
  - Kmax/Kmin summaries, retrieval scoring, update/refresh/reset.
- **[x]** `initSpecPVFromFullKV` flattens full KV prefix and seeds partial
          KV cache.
- **[x]** `runEagleTargetTreeDecode` can run tree decode against partial KV:
  - builds scratch KV blocks from `PartialKVCache::active_prefix`,
  - uses scratch KV for prefix + tree; live KV untouched.
- **[x]** `updateSpecPVAfterAcceptance`:
  - reads committed tail lengths from `ForwardMultiStep`,
  - calls `PartialKVCache::update_after_acceptance` per layer/slot,
  - robustly disables SpecPV on any invariant failure.

**TODOs (after Eagle‑3 core is solid):**

- [ ] **6.1 Validate semantics with EAGLE‑3**
  - For long‑context runs:
    - verify token streams with SpecPV enabled vs disabled are
      identical (within numerical noise),
    - confirm EAGLE metrics (acceptance lengths, EOS handling) are
      unchanged by SpecPV.

- [ ] **6.2 Tune SpecPV geometry for GPT‑OSS‑120B‑Eagle3**
  - Choose default `specpv_*` parameters (block sizes, retrieval window,
    buffer) that:
    - give a measurable reduction in KV tokens attended during tree
      decode at long context,
    - maintain acceptance metrics close to full‑KV runs.

- [ ] **6.3 Perf benchmarks with SpecPV**
  - Extend the speculative benchmark suite with SpecPV‑enabled runs:
    - throughput vs full‑KV Eagle‑3 at 32K/64K context,
    - memory usage and KV bandwidth comparisons.

When all the above phases are checked off, TurboMind’s EAGLE‑3 path
should be functionally, numerically, and performance‑wise aligned
with TensorRT‑LLM Eagle‑3 on GPT‑OSS‑120B, with SpecPV as an optional
long‑context optimisation layer on top.
