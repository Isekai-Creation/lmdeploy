# TurboMind EAGLE3 – Final TODO Snapshot (Code State as of Current Branch)

This file reflects the **current implementation** in this repo and the
**remaining coding work** needed to get as close as practical to
TensorRT‑LLM Eagle‑3 behaviour on TurboMind.

It is implementation‑only: no process, no docs, no tests.

Status markers:
- `[x]` implemented in this tree (may still need tuning)
- `[ ]` not implemented / still to do

---

## 0. High‑Level Progress (per area)

- Target‑tree decode (TurboMind target side):
  - **[x]** Prepared tree inputs (`targetTreeDecode` → `EagleBuffers::inputs.eagle_net_*`).
  - **[x]** Scratch KV tree decode with prefix reuse (`runEagleTargetTreeDecode`).
  - **[x]** Packed tree masks in attention (`spec_packed_mask` → `UnifiedAttentionLayer`).
  - **[x]** Tree logits → per‑node `target_tokens` (device argmax + scatter).

- Acceptance + multi‑token tails + KV rewind:
  - **[x]** Device tree acceptance (`invokeTreeAcceptByIdsWithPaths`, strict ID equality).
  - **[x]** Tail commit via `DynamicDecodeLayer::ForwardMultiStep`
           (EOS, seq_limit_len / max_new_tokens, per‑slot finished).
  - **[x]** Multi‑token metrics + KV rewind driven by committed tail lengths.

- Eagle‑3 converter, geometry, capture + FC:
  - **[x]** Converter exports Eagle‑3 FC/QKV/WO/MLP/norms/embeddings/LM head
           with correct geometry into draft dir + `config.yaml`.
  - **[x]** `EagleModule::load` enforces Eagle‑3 geometry and builds
           `Eagle3DraftLayerWeight` with strict shape checks and local fallbacks.
  - **[x]** UnifiedDecoder capture width matches `eagle_fc_in_dim`; FC+norm path
           is enforced and locally disabled per engine on mismatch.

- Eagle‑3 draft layer under UnifiedDecoder:
  - **[x]** `Eagle3DraftLayerWeight` (attn + FFN + norms) built from converter exports.
  - **[x]** `Eagle3DraftLayer::ForwardDraft` path exists:
           RMSNorm → shallow fused QKV+Wo → RMSNorm → GatedMLP → residual+output RMSNorm.
  - **[x]** `UnifiedDecoder::ForwardDraft` uses `Eagle3DraftLayer` when available.
  - **[x]** `LlamaV2::runEagle3DraftTreeDecode` drives draft logits and fills
           `EagleBuffers::inputs.draft_tokens/target_tokens`.
  - **[x]** For `spec_method == "eagle3"`, `dynamicDecodeWithSpecMulti` uses
           `runEagle3DraftTreeDecode` exclusively; EagleModule draft is used only
           for non‑Eagle‑3 modes.
  - **[ ]** Attention in `Eagle3DraftLayer` is still linearised (no real softmax
           attention over KV) – this is the main remaining numerics gap vs TRT‑LLM.

- SpecPV partial KV:
  - **[x]** `SpecPVCacheConfig` + `PartialKVCache` implemented (`specpv_kv_cache`).
  - **[x]** Seeding from full KV (`initSpecPVFromFullKV`) and partial/full
           switching policy in `LlamaV2`.
  - **[x]** Tree decode can run against partial KV in SpecPV mode.
  - **[x]** Post‑acceptance partial KV update wired to EAGLE committed tail
           lengths via `PartialKVCache::update_after_acceptance` and
           `LlamaV2::updateSpecPVAfterAcceptance`, with per‑layer/slot guards.

---

## 1. Eagle‑3 Draft Layer – Remaining Work

### 1.1 Replace shallow attention with real multi‑head attention

**Where:**
- `src/turbomind/models/llama/EagleDraftLayer.{h,cc}`
- `src/turbomind/models/llama/unified_attention_layer.{h,cc}` / `LlamaAttentionLayer` (reference)

**Current:**
- `Eagle3DraftLayer::Forward` does:
  - `input_norm` RMSNorm on `input_hidden`.
  - Fused `attn.qkv.weight` matmul, slices a Q‑like chunk, then Wo matmul.
  - No softmax, no Q/K/V splitting, no attention over a KV sequence.

**TODO:**
- [ ] Use the same attention backend as the main decoder:
  - Build `AttentionParams` for a single position per slot:
    - `batch_size = input_hidden.shape(0)`
    - `q_len = kv_len = 1`
    - `head_num`, `kv_head_num`, `size_per_head` from Eagle‑3 geometry.
    - RoPE / scaling copied from `UnifiedDecoder::Forward`.
  - Split Q/K/V from `attn.qkv.weight` exactly as LLaMA attention does.
  - Call the attention kernel (`UnifiedAttentionLayer::Forward` or `LlamaAttentionLayer::forward`)
    to get `attn_out [B,H]`.
- [ ] Keep existing FFN + residual + output RMSNorm on top of this attention output:
  - `ffn_input = RMSNorm(attn_out, post_attn_norm)`
  - GatedMLP via `LlamaFfnLayer`
  - `output_hidden = RMSNorm(input_hidden + ffn_out, output_norm)` (already uses
    `invokeResidualBiasRMSNorm` – keep that).

**Guards:**
- [ ] If attention cannot be set up (geometry mismatch, invalid head config):
  - Log `[EAGLE3][Draft] invalid attention geometry; skipping Eagle3DraftLayer::Forward`.
  - Set `output_hidden = input_hidden` and `return`.
- Do **not** touch `spec_mode_` or global EAGLE flags here.

### 1.2 Confirm Eagle‑3 draft source is solely `UnifiedDecoder::ForwardDraft`

**Where:**
- `src/turbomind/models/llama/LlamaV2.cc`
- `src/turbomind/models/llama/EagleModule.cc`

**Status:**
- [x] For `engine_param_.spec_method == "eagle3"`, `dynamicDecodeWithSpecMulti`
     calls `runEagle3DraftTreeDecode`, which uses `UnifiedDecoder::ForwardDraft`
     + LM head + GPU Top‑K to populate `EagleBuffers::inputs.draft_tokens/target_tokens`.
- [x] `EagleModule::forward_draft_tree` is used only for non‑Eagle‑3 modes.

**TODO (cleanup):**
- [ ] Re‑scan Eagle‑3 code paths and delete any remaining Eagle‑3‑specific draft
      usage inside `EagleModule` (keep only the legacy EagleNet draft paths, gated
      by `eagle_mode_`).

### 1.3 Expose Eagle‑3 draft intermediates cleanly for HF/TRT comparison

**Where:**
- `src/turbomind/models/llama/EagleDraftLayer.{h,cc}`
- `src/turbomind/python/bind.cpp`
- `lmdeploy/tests/turbomind/eagle3_compare.py`

**Status:**
- [x] `_turbomind.eagle3_forward_debug` runs the Eagle‑3 draft layer + LM head and
     returns logits.
- [x] `eagle3_compare.py` already consumes a debug dict and computes stage metrics.

**TODO:**
- [x] In `Eagle3DraftLayer`, add optional debug tensors (only when a global
      eagle‑debug flag is on) for:
  - `attn_out` – attention output before FFN.
  - `ffn_out` – GatedMLP output before final norm.
  - `pre_head_hidden` – post‑`output_norm` hidden.
- [x] In `_turbomind.eagle3_forward_debug`:
  - When Eagle‑3 draft layer is present, return:
    - `logits`,
    - `attn_out`, `ffn_out`, `pre_head_hidden` pulled from `Eagle3DraftLayer`.
- [ ] In `eagle3_compare.py`:
  - For each stage (ATTN_OUT, FFN_OUT, PRE_HEAD, LOGITS):
    - Compute `mean_abs_diff`, `max_abs_diff`, cosine similarity vs HF.
    - For LOGITS: argmax match rate and top‑k overlap.
  - Use this as the numeric loop to tune capture ordering, QKV input composition,
    and any remaining norm placements.

### 1.4 Iterate numerically until draft logits are usable

**Where:**
- `lmdeploy/tests/turbomind/eagle3_compare.py`
- `TensorRT-LLM/` reference (`Eagle3DecoderLayer`, `Eagle3DraftModel`)

**TODO:**
- [ ] For GPT‑OSS‑120B‑Eagle3, adjust only when stage metrics show clear
      mismatch:
  - capture layer indices/order vs HF,
  - QKV input composition (`[embed_norm, fc_norm]`, etc.),
  - norm placements (only if absolutely necessary).
- [ ] Stop when:
  - `argmax_match_rate > 0` and non‑trivial top‑k overlap,
  - LOGITS cosine similarity is reasonable for MXFP4/BF16 vs HF.

### 1.5 Final Eagle‑3 draft cleanup

**TODO:**
- [ ] Once the new Eagle‑3 draft layer is stable:
  - Remove all Eagle‑3‑specific shallow QKV+Wo draft paths in `EagleModule`.
  - Ensure:
    - For Eagle‑3, all tree drafts come from `UnifiedDecoder::ForwardDraft`
      + LM head.
    - `EagleModule` is only used for non‑Eagle‑3 (legacy EagleNet) draft modes.

---

## 2. SpecPV Partial KV – Remaining Work

### 2.1 Implement explicit post‑acceptance update in `PartialKVCache`

**Where:**
- `src/turbomind/models/llama/specpv_kv_cache.{h,cc}`

**Status:**
- [x] `SpecPVCacheConfig` and `PartialKVCache` exist, with:
  - K/V segments (sink, retrieval, window, buffer),
  - summary Kmax/Kmin, retrieval scoring, update/refresh/reset.

**Status / behaviour (implemented):**
- **[x]** `PartialKVCache::update_after_acceptance(int layer_idx,
       int slot, int advance_tokens)` is implemented and used to advance
       `verified_lens_[layer_idx]` and recompute `global_verified_len_`.
  - On invalid `layer_idx`, zero `buffer_size()`, negative current length,
    or buffer overflow, it logs a `[SpecPV][fallback] ...` message,
    disables SpecPV for the engine (`enabled_ = false`) and resets
    `global_verified_len_` to 0.

### 2.2 Wire `update_after_acceptance` into the EAGLE multi‑token path

**Where:**
- `src/turbomind/models/llama/LlamaV2.cc`
- `src/turbomind/layers/DynamicDecodeLayer.cc` (for `committed_lengths`)

**Status / behaviour (implemented):**
- **[x]** After `dynamicDecodeWithSpecMulti` commits tails, `LlamaV2`
  calls `updateSpecPVAfterAcceptance`, which:
  - Reads per‑slot committed tail lengths (when available) or falls back
    to `advance = 1` for active single‑token steps.
  - For each slot and each layer, calls
    `specpv_kv_cache_->update_after_acceptance(layer, i, advance)` when
    SpecPV is enabled.
  - If the cache disables itself during these updates, logs
    `[SpecPV][fallback] PartialKVCache update_after_acceptance failed; disabling SpecPV for this engine.`
    and clears `specpv_kv_cache_` / related flags without affecting
    EAGLE or baseline KV.

### 2.3 Keep partial/full switching disciplined

**Where:**
- `LlamaV2` SpecPV helpers (`isSpecPVEnabled`, `shouldUseSpecPV`, `initSpecPVFromFullKV`, refresh logic)

**Status / remaining checks:**
- **[x]** `shouldUseSpecPV`, `specpv_partial_steps_`, and headroom thresholds
  already:
  - Gate entry into SpecPV based on sequence length,
  - Force full‑KV tree decode when partial steps or buffer headroom are
    exhausted,
  - Reseed the partial KV from full KV via `initSpecPVFromFullKV`,
    and use `update_after_acceptance` to track buffer‑local tails.
- **[ ]** When tuning SpecPV further, keep verifying that no EAGLE semantics
  (tree structure, acceptance, EOS/stop handling) are changed – SpecPV
  must only affect which KV entries the target attends to during
  target‑tree decode.

---

## 3. Quick Numeric/Perf Checklist Before Calling It “Done”

Once the items above are implemented, the final validation loop should be:

- [ ] `tests/turbomind/eagle3_compare.py`:
  - Stage stats for Eagle‑3 draft:
    - ATTN_OUT, FFN_OUT, PRE_HEAD, LOGITS vs HF Eagle‑3.
  - `argmax_match_rate` and top‑k overlap > 0 for LOGITS.

- [ ] `inference/benchmark_speculative.py` / `run_spec_suite.sh`:
  - For GPT‑OSS‑120B + Eagle‑3 draft:
    - `mean_acceptance_length > 1`.
    - `fraction_steps_accept_ge2 > 0`.
    - Throughput ≥ baseline (non‑speculative) in key scenarios.

- [ ] SpecPV‑enabled runs (long context only):
  - Same acceptance stats as full‑KV runs.
  - Lower target‑tree decode cost at long context (fewer KV tokens seen per step).

At that point, TurboMind’s Eagle‑3 speculative decoding is structurally and
numerically aligned as far as is reasonable with TensorRT‑LLM, with SpecPV
providing an additional, optional verification optimisation on top.
