# TurboMind EAGLE‑3 – Design & TODO Checklist

This file is the **implementation + design TODO** for bringing TurboMind’s
EAGLE‑3 path to **production‑grade parity** with TensorRT‑LLM on
**GPT‑OSS‑120B‑Eagle3**.

Status markers:
- `[x]` implemented in this tree (may still need tuning)
- `[ ]` not implemented / still to do

Current progress (no stubs): **~89/100** toward full TRT‑LLM parity.
- Geometry/export/shape guards are locked (B done).
- Draft attention now runs real Q/K/V GEMMs + RoPE params + offset-aware SDPA + Wo, with q_len/kv_len/past_kv_len/packed_mask_stride/positions/runtime offsets/kv_lens_runtime plumbed; TRT RoPE math, tree mask parity, KV reuse, and debug validation still pending (A in progress).
- Tree decode has per-node logits scaffold, packed-mask fallback, successor offsets/counts on device with validation, runtime offsets threaded into attention, and kv_lens_runtime propagated; successor/topK parity, acceptance alignment, and validation still pending (C in progress).

Roles (for reference):
- **A – Kernels & math** (attention, RoPE, KV layout, CUDA)
- **B – Model & converter** (HF → draft dir, geometry, config)
- **C – TurboMind integration** (LlamaV2, UnifiedDecoder, KV, LMDeploy)
- **D – Validation & infra** (tests, debug harnesses, benchmarks)

## Engineer Task Matrix (blocking items to completion)

- **Engineer A – Kernels & math (CUDA/attention)**
  - **State:** Q/K/V GEMMs + RoPE params + offset-aware naive SDPA + Wo with q_len/kv_len/past_kv_len/packed_mask_stride/positions/runtime offsets plumbed; UnifiedAttention gated off for non-Llama geometries.
  - **Next:** Implement full SDPA parity (q_len/kv_len>1) with causal + tree masks and TRT RoPE math; enforce true 2H packed input (embedding_norm + FC_norm); mirror scaling/clamping/KV reuse; expose debug tensors/CPU ref and remove shallow fallback on valid geometry. Primary refs: `cpp/tensorrt_llm/kernels/speculativeDecoding/eagleDecodingKernels.cu`, `draftTokenTreeKernels.cu`, `tensorrt_llm/_torch/speculative/eagle3.py`, `modeling_speculative.py`.

- **Engineer B – Converter & model geometry**
  - Lock GPT‑OSS‑120B Eagle‑3 geometry: base_hidden vs draft_hidden, q/k/v/o shapes, head counts/dims, eagle_fc_in_factor; document in design doc and enforce in converter.
  - Strengthen `eagle_draft_converter.py` checks: fail fast on HF layout drift; ensure Wo is padded to base_hidden, QKV in uses 2*draft_hidden, norms/FFN use draft_hidden. Add unit tests under `tests/turbomind` to read back `config.yaml` and weights and assert shapes/dtypes.
  - Confirm capture ordering (`layers_to_capture`) matches EAGLE/TRT; align `eagle_capture_hidden_` and `eagle_fc_in_dim_` with converter outputs. Update docs accordingly.
  - Keep LMDeploy config parity with TRT‑LLM (eagle_q_size, eagle_kv_size, eagle_qkv_in_dim, eagle_fc_in_dim, eagle_choices, max_total_draft_tokens).
  - [x] Converter exports geometry/capture layers (1,17,32), enforces qkv_in=2*draft_hidden; shape unit added (`tests/turbomind/test_eagle_converter_shapes.py`).
  - **State:** Geometry/config scope complete; keep CI guard running. Await A/C to land real attention/tree decode to delete shallow path.

- **Engineer C – Integration, tree decode, validation**
  - **State:** Per-node draft logits scaffolded; packed masks with fallback; successor offsets/counts on device with host/device validation; runtime offsets threaded into attention; lengths clamped and synced. Successor/topK parity, kv_lens parity, acceptance, and validation still pending.
  - **Next:** Finish F2 by porting successor/topK/offset prep and kv_lens runtime parity from TRT (`eagleDecodingKernels.cu`, `draftTokenTreeKernels.cu`, `eagleDecodingLayer.cpp`); wire offsets/masks into attention/acceptance; align acceptance semantics; coordinate with A to pass per-node positions/masks. Add stagewise compare/benchmarks once kernels land.

Handoff (A/C) after geometry:
- Converter/config now export base_hidden vs draft_hidden, qkv_in=2*draft_hidden, capture layers [1,17,32]; shape guard `tests/turbomind/test_eagle_converter_shapes.py` (CPU).
- EagleModule enforces Eagle3 shapes; draft layer norms/FFN run in draft space and warn when shallow pass-through is active. Remove shallow path once real Eagle3 attention lands.
- Remaining gaps: real Eagle3 attention kernels (Q/K/V+RoPE+SDPA), 2H input packing parity (emb_norm+fc_norm), per-node draft tree decode/acceptance, stagewise compare + spec benchmarks.

## Engineer Detailed Task Checklist (to 100% completion)

### Engineer A – Kernels & math
- [ ] Implement `Eagle3AttentionWeight` construction and metadata (q heads, kv heads, head dims, scaling).
- [ ] Replace TODO in `Eagle3AttentionLayer::Forward` with full CUDA path: Q/K/V GEMMs, reshape, RoPE, SDPA (masking), V aggregation, O GEMM, output assign; no stubs. **Current:** Q/K/V GEMM + RoPE params + offset-aware naive SDPA + Wo with q_len/kv_len/past_kv_len/packed_mask_stride/positions/runtime offsets plumbed; **missing:** tree/causal mask parity, KV reuse, true 2H packing enforcement, multi-token.
- [ ] Mirror TRT‑LLM scaling/clamping exactly: 1/sqrt(d_k), max-subtraction, dtype handling (BF16/FP16), apply attention mask semantics identical to TRT tree masks.
- [ ] Compose true 2H QKV input (embedding_norm + fc_out mapped/padded to base_hidden) in draft forward; remove placeholder composition and head_num*size_per_head gating; match TRT packing order.
- [ ] Ensure draft attention uses real backend when geometry matches; log explicit fallback otherwise. Remove shallow path for valid Eagle‑3 configs (no demo shortcuts).
- [ ] Add per-node draft attention support if TRT uses q_len>1; otherwise document single-position semantics and guard. Include KV reuse across tree tokens as TRT does.
- [ ] Implement mask-aware kernels for tree attention (packed masks from `draft_paths`) matching TRT behaviour for multi-token speculative decoding.
- [ ] Expose debug tensors (fc_out, attn_out, ffn_out, pre_head_hidden, logits) and a small-batch CPU reference for cross-checks; keep fidelity to TRT values.
- [ ] Validate KV reuse semantics in tree decode (scratch KV, prefix read-only) vs TRT; fix any layout mismatch so acceptance outcomes align.
- [ ] Optimize kernels after correctness: benchmark vs TRT Eagle‑3, ensure equal or better throughput without regressing numerics.
- [ ] Production-ready hardening: add error checks on stream/cublas status, guard against shape/dtype mismatches at runtime, and ensure kernels handle dynamic batch/tree sizes without silent failure.
- [ ] Cross-verify against current TRT‑LLM code/tests: head geometry, RoPE axes/scale, tree mask semantics, q_len handling; update kernels/doc immediately on drift.
- [ ] Read and mirror TensorRT‑LLM sources: `tensorrt_llm/_torch/speculative/eagle3.py`, `tensorrt_llm/_torch/models/modeling_speculative.py` (Eagle3Attention), attention backend in `tensorrt_llm/_torch/attention_backend/trtllm.py`, and unit tests `tests/unittest/_torch/speculative/test_eagle3.py` to capture exact math, masking, and indexing. Document any discovered quirks (KV padding, scaling tweaks) and reflect them in kernels.
- [ ] GPT‑OSS‑120B‑Eagle3 specifics (from HF ckpt + TRT): draft hidden 2880, target hidden 2880, Q_proj [4096, 5760], K/V_proj [512, 5760], O_proj [2880, 4096]; num_heads=64, num_kv_heads=8, head_dim=64 for Q, kv_head_dim=64. Ensure kernels use these exact shapes and stride assumptions.

### Engineer B – Converter & geometry
- [ ] Lock Eagle‑3 geometry for GPT‑OSS‑120B: base_hidden, draft_hidden, q/k/v/o shapes, head counts/dims, eagle_fc_in_factor; document in design doc.
- [ ] Enforce geometry in `eagle_draft_converter.py`: Wo padded to base_hidden, QKV input uses 2*draft_hidden, norms/FFN on draft_hidden, head metadata exported.
- [ ] Add fail-fast checks for HF layout drift (q/k/v/o, fc shapes, capture layers) with clear errors.
- [ ] Add unit tests (`tests/turbomind`) to run converter on sample checkpoint/subset, read `config.yaml` and weights, assert shapes/dtypes for all exported tensors.
- [ ] Confirm capture ordering (`layers_to_capture`) matches EAGLE/TRT; align `eagle_capture_hidden_`, `eagle_fc_in_dim_`, and design doc.
- [ ] Keep LMDeploy config parity: expose eagle_q_size, eagle_kv_size, eagle_qkv_in_dim, eagle_fc_in_dim, eagle_choices, max_total_draft_tokens; validate parsing.
- [ ] Update docs with final geometry, converter behaviour, and failure modes.
- [ ] Production-ready hardening: deterministic converter outputs, checksum/logging of exported weights, and CI guard to fail on shape drift across checkpoints.
- [ ] Cross-check geometry and capture ordering directly against TRT‑LLM checkpoints/configs and unit tests; if mismatch, fix converter and docs before release.
- [ ] Read TRT‑LLM configs/checkpoints for GPT‑OSS‑120B‑Eagle3 to confirm `head_num`, `kv_head_num`, `head_dim`, FC fan-in/out, and capture layers; sync converter and `config.yaml` fields to those exact values.
- [ ] Encode GPT‑OSS‑120B‑Eagle3 geometry: q_proj [4096, 5760], k_proj/v_proj [512, 5760], o_proj [2880, 4096], fc.weight [2880, 8640]; head_num=64, kv_head_num=8, head_dim=64; draft_hidden=2880, qkv_in=5760 (2*draft_hidden), fc_in=8640 (3*draft_hidden). Enforce these in converter and config.
- [ ] Capture layers per TRT defaults: for target 36 layers, Eagle3 capture indices (1, 17, 32) as in config `eagle_aux_hidden_state_layer_ids`; ensure converter writes `num_capture_layers=3` and matches ordering used in TRT.

### Engineer C – Integration, tree decode, validation
- [ ] Thread base_hidden/draft_hidden through `EagleModule`, `Eagle3DraftLayer`, `UnifiedDecoder`, `LlamaV2`; ensure allocations/norms/FFN use draft_hidden, attention/O use base_hidden.
- [ ] Route draft attention to new Eagle3 backend when geometry matches; keep strict `[EAGLE3][Attention][fallback]` logging for invalid cases only.
- [ ] Extend draft tree decode to per-node logits for acceptance; update `runEagle3DraftTreeDecode` / acceptance kernels to consume per-node outputs. If deferring, add warning/log and plan.
- [ ] Align tree masks/acceptance with TRT semantics (static tree, branching, root token handling, per-node KV slots); document any intentional differences. Match TRT multi-token speculative behaviour end-to-end.
- [ ] Add stagewise compare harness (`tests/turbomind/eagle3_compare.py`) pulling HF/TRT refs; compute cosine/max diff/top‑k overlap for fc_out, attn_out, ffn_out, pre_head_hidden, logits; set tolerances.
- [ ] Add Eagle‑3 profile to `run_spec_suite.sh` / `benchmark_speculative.py`; record acceptance metrics (mean length, frac>=2) and throughput vs TRT; fail if below TRT parity targets.
- [ ] Validate dynamic batching/block reuse/KV invariants under spec: cache_len, block manager consistency, KV rewind correctness.
- [ ] Clean up legacy shallow paths once parity proven; centralize `spec_method="eagle3"` handling; ensure LMDeploy surfaces TRT-like options (choices, max_total_draft_tokens, SpecPV toggle).
- [ ] Validate SpecPV long-context behaviour vs full KV with Eagle‑3; log/disable on invariant failure.
- [ ] Update `EAGLE_TODO_FINAL` and design doc once milestones are green; maintain progress markers.
- [ ] Production-ready hardening: add regression tests for acceptance/KV invariants, enable debug counters for attention fallback frequency, and document operational runbooks for Eagle‑3 in LMDeploy (config knobs, known limits).
- [ ] Define explicit tolerances/benchmarks (cosine/logit overlap, acceptance lengths, throughput) pre-run; enforce CI failures on violations. Keep doc synchronized with any newly observed TRT behaviours (KV padding, scaling quirks, tree masks).
- [ ] Read TRT‑LLM speculative path: `tensorrt_llm/_torch/speculative/drafting_loops.py`, `tensorrt_llm/models/eagle/model.py`, `tests/unittest/_torch/speculative/test_kv_cache_reuse.py` to replicate tree construction, masks, acceptance, KV slotting, and q_len semantics exactly; update TurboMind tree decode/acceptance to match.
- [ ] For GPT‑OSS‑120B‑Eagle3: apply static tree masks as TRT (packed mask via powers-of-two, position offsets contiguous), generation_length=max_total_draft_tokens+1 for target; first draft layer uses padded max_draft_len+1 tokens when static tree; linear tree requires max_draft_len==max_total_draft_tokens. Match kv_lens rewinding and position_id updates per TRT drafting loop (q_len=1 per step).
- [ ] KV semantics: ensure `kv_lens_runtime` excludes `num_extra_kv_tokens` (max_draft_len-1) while internal `kv_lens` includes them, mirroring TRT test `test_kv_lens_runtime_with_eagle3_one_model`; add regression.
- [ ] Mirror TRT plugins/acceptance: replicate behaviour of `eagle_sample_and_accept_draft_plugin` and `eagle_draft_decoder_plugin` (tree paths, posterior thresholds, greedy vs multinomial) or document deviations; ensure acceptance outputs (accepted_tokens/paths/lens) match TRT for GPT‑OSS‑120B static tree.
- [ ] Mirror TRT C++ tree prep kernels: implement equivalents of `prepareCtxEagleNetInputs`, `buildLeafMask`, `getNonLeafEndingSubtree`, `prepareGenEagleNetInputs`, `getPackedMask`, `getPackedMaskFromPath`, `assembleTarget/DraftLogitsOffsets`, successor counting/topK from `eagleDecodingKernels.cu`; ensure hidden-state indices, lastTokenIndices, packed masks, position offsets match TRT layouts and padding (batch<=512 constraint noted).
- [ ] Align orchestration with `eagleDecodingLayer.cpp`: call order and buffer layouts for static/dynamic tree, mask/position prep, acceptance integration; ensure TurboMind module/buffers match `runtime/eagleModule.h` expectations.
- [ ] Audit TurboMind vs TRT pipeline end-to-end: capture→FC→Eagle3 attn→FFN→LM head→tree build→draft logits→acceptance→target tree decode→KV rewind. For each stage, mark implemented vs missing vs stub; ensure no shallow fallback remains in normal Eagle‑3 configs. Document any intentional deviations.
- [ ] Align with TRT stack map (porting targets):
  - Runtime API/headers: `cpp/include/tensorrt_llm/runtime/eagleModule.h`, `eagleBuffers.h`, `explicitDraftTokensBuffers.h`, `lookaheadBuffers.h`, `lookaheadModule.h`, `speculativeDecodingModule.h`, `speculativeDecodingMode.h`, `decodingInput.h`, `decodingOutput.h`, `utils/speculativeChoicesUtils.h`; executor configs `executor/speculativeDecodingConfig.cpp`, `executor/decodingConfig.cpp`, `executor/guidedDecodingConfig.cpp`.
  - Kernels: `kernels/speculativeDecoding/common.{h,cu}`, `draftTokenTreeKernels.{h,cu}`, `mtpKernels.{h,cu}`, `eagleDecodingKernels.{h,cu}`, `kvCacheUpdateKernels.{h,cu}`, plus medusa/explicit/external draft kernels for patterns.
  - Layers (host orchestration): `layers/eagleDecodingLayer.{h,cpp}`, `explicitDraftTokensLayer.{h,cpp}`, `externalDraftTokensLayer.{h,cpp}`, `medusaDecodingLayer.{h,cpp}`, `lookaheadDecodingLayer/Algorithm/PoolManager`, `dynamicDecodeLayer.{h,cpp}`, `decodingLayer.{h,cpp}`.
  - Plugins: `plugins/eaglePlugin/eaglePrepareDrafterInputsPlugin.{h,cpp}`, `eagleDecodeDraftTokensPlugin.{h,cpp}`, `eagleSampleAndAcceptDraftTokensPlugin.{h,cpp}` (bridge to kernels); study plugin wiring/inputs/outputs.
  - Runtime impl: `runtime/eagleBuffers.cpp`, `explicitDraftTokensBuffers.cpp`, `decodingLayerWorkspace.{h,cpp}`, `utils/speculativeChoicesUtils.cpp`, `runtime/gptDecoder.{h,cpp}`, `runtime/gptDecoderBatched.{h,cpp}`, batch manager KV (`batch_manager/kvCacheManager.cpp`, `kvCacheUtils.h`, `guidedDecoder.{h,cpp}`).
  - Torch ops: `thop/specDecOp.cpp`, `thop/dynamicDecodeOp.{h,cpp}`, `thop/attentionOp.*` for exposure/testing.
  - Tests/resources: `tests/unit_tests/layers/eagleLayerTest.{cpp,h}`, `explicitDraftTokensLayerTest.{cpp,h}`, `externalDraftTokensLayerTest.{cpp,h}`, `medusaDecodeLayerTest.{cpp,h}`, `tests/unit_tests/kernels/eaglePackDataTest.cpp`; scripts `tests/resources/scripts/build_eagle_engines.py`, `generate_expected_eagle_output.py`, `build_gpt_engines.py`.
  - Docs: `docs/source/blogs/tech_blog/blog11_GPT_OSS_Eagle3.md`, blog9, blog6, blog12, deployment guide for GPT-OSS on TRT-LLM, `docs/source/commands/trtllm-build.rst`. Mirror config knobs/runtime behaviour accordingly.
  - Assignments: Engineer A owns kernels/math parity; Engineer B owns converter/config/geometry; Engineer C owns layers/plugins/runtime integration, tree logic, KV/cache, and validation.
- [ ] Pipeline parity checklist (fill with IMPLEMENTED/MISSING): capture layers order/concatenation; FC geometry (3H→H); 2H QKV compose; Eagle3 attention (QKV/O shapes, RoPE, scaling, masking); FFN/norms on draft_hidden; LM head routing; tree construction (paths/masks/offsets/padding); draft logits assembly; acceptance plugin semantics; target tree decode scratch KV + masks; KV rewind/num_extra_kv_tokens handling; dynamic batching/block reuse invariants; SpecPV optional path. Update this checklist as work lands; no stubs.

**Pipeline parity status (current, do not fake; update as you implement)**
- capture layers order/concatenation: PARTIAL (capture plumbing present; enforce TRT ordering 1/17/32 + doc).
- FC geometry (3H→H): IMPLEMENTED in converter; runtime uses draft_hidden allocations.
- 2H QKV compose: MISSING (still placeholder/tiled; must pack embedding_norm + fc_out into 2*draft_hidden exactly like TRT).
- Eagle3 attention (QKV/O shapes, RoPE, scaling, masking): PARTIAL (Q/K/V GEMM + RoPE params + offset-aware SDPA + Wo with q_len/kv_len/past_kv_len/positions/runtime offsets; missing tree/causal mask parity, KV reuse, multi-token RoPE).
- FFN/norms on draft_hidden: IMPLEMENTED allocations; verify residual/norm paths stay draft_hidden.
- LM head routing: IMPLEMENTED (base_hidden×vocab); stagewise compare pending.
- tree construction (paths/masks/offsets/padding): PARTIAL (tree + packed-mask fallback; successor/topK offsets and kv_lens parity pending).
- draft logits assembly (per-node, offsets): PARTIAL (per-node logits scaffolded; acceptance wiring pending).
- acceptance plugin semantics: MISSING (need behaviour match to `eagle_sample_and_accept_draft_plugin`/decoder plugin).
- target tree decode scratch KV + masks: IMPLEMENTED in TurboMind, not validated against TRT masks/offsets.
- KV rewind/num_extra_kv_tokens handling: PARTIAL (kv_lens_runtime vs internal extra tokens parity not enforced).
- dynamic batching/block reuse invariants: MISSING validation under Eagle-3.
- SpecPV optional path: IMPLEMENTED plumbing, UNVALIDATED with Eagle-3.

---

## 0. Scope & Success Criteria

**Functional parity (vs TensorRT‑LLM Eagle‑3 on GPT‑OSS‑120B)**
- [ ] Multi‑token EAGLE‑3 draft tree decode (static tree for GPT‑OSS‑120B).
- [ ] **Real Eagle‑3 attention** in the draft head
      (no “shallow QKV hack” in normal configs).
- [ ] Correct acceptance, tail commit, and KV reuse under:
      - dynamic batching,
      - block reuse / block manager semantics,
      - mixed prefill + decode (offline pipeline).

**Numeric parity**
- [ ] Stage‑wise draft numerics for GPT‑OSS‑120B‑Eagle3:
      - ATTN_OUT / FFN_OUT / PRE_HEAD / LOGITS:
        TM vs HF/TRT within agreed tolerances (cosine, max diff).
- [ ] Speculative acceptance behaviour consistent with TRT‑LLM
      for the **same model + prompts** (distribution of accepted lengths,
      EOS/stop behaviour).

**Performance parity**
- [ ] Mean acceptance length in the **4–7 tokens** range on
      GPT‑OSS‑120B‑Eagle3 workloads comparable to TRT‑LLM.
- [ ] Throughput improvement vs baseline TurboMind (no spec) is
      **≥ TRT‑LLM Eagle‑3** on the same hardware class (H100/B200/GB200)
      for representative prompts (8K/32K context).

**Integration**
- [x] Works in **offline pipeline** (TurboMind engine via LMDeploy).
- [ ] Coexists cleanly with:
      - non-speculative decode,
      - other speculative methods (vanilla, lookahead, Medusa-style),
      - **SpecPV** (partial KV) as an optional long-context optimisation.

---

## Engineer C – Next 10 Implementation Tasks (each ~10–15% of remaining work)

1. **Geometry split in C++**: propagate base_hidden (attention/output) vs draft_hidden (FC/MLP) through EagleModule/Eagle3DraftLayer; allocate norms/FFN/LM head on draft_hidden, attn Wo on base_hidden, QKV input on 2*draft_hidden; soft-disable draft layer on geometry mismatch.
2. **Converter/config alignment**: ensure `config.yaml` carries base/draft hidden, qkv/fc factors (`eagle_qkv_in_dim=2*draft_hidden`, `eagle_fc_in_dim=3*draft_hidden`), and dtype; enforce shapes when exporting fused QKV/Wo.
3. **Eagle3DraftLayer Forward 2H input**: build true 2×draft_hidden QKV input from embedding_norm + Eagle-3 FC capture (TRT-LLM composition); fallback to tiled last_hidden only if capture/embeddings absent.
4. **Real attention path**: wire UnifiedAttentionLayer/Eagle3AttentionLayer for q_len=kv_len=1, no KV cache; shallow linear fallback only when QKV/Wo invalid; remove head_num*size_per_head gating.
5. **Stagewise compare harness**: extend `tests/turbomind/eagle3_compare.py` to pull HF draft FC/attn/FFN/pre/logits and compare to turbomind debug tensors via dlpack; print cosines/argmax/top5.
6. **Tree draft per-node**: add per-node draft logits in UnifiedDecoder/LlamaV2 using existing tree flattening; fill draft_tokens from per-node device logits; if deferred, log the current per-slot limitation explicitly.
7. **Gating/cleanup**: remove legacy shallow Eagle-3 paths; centralize `spec_method == "eagle3"` in LlamaV2 with per-step soft fallback only; keep SpecPV/EAGLE metrics consistent.
8. **SpecPV validation**: rerun long-context (16k/32k) baseline/Eagle3/Eagle3+SpecPV; add targeted logging in PartialKVCache::update_after_acceptance and LlamaV2::updateSpecPVAfterAcceptance for inconsistencies.
9. **Benchmarks**: run `run_spec_suite.sh` post-geometry fixes; track mean/max acceptance, fraction_accept_ge2, mean_committed_extras, throughput vs baseline.
10. **Docs/cleanup**: update design doc and this TODO after validation; ensure legacy shallow Eagle3 is removed and final gating is single-source.

---

## 1. Phase 0 – Spec Extraction & Design Alignment (TRT‑LLM + EAGLE)

Goal: a short **design doc** that is the source of truth for the rest of the
work. No further refactors until this is written and reviewed.

**Where to read (TRT‑LLM side):**
- `tensorrt_llm/models/eagle/model.py`
- `tensorrt_llm/_torch/speculative/eagle3.py`
- `tensorrt_llm/_torch/speculative/drafting_loops.py`
- `tensorrt_llm/_torch/models/modeling_speculative.py`
  (Eagle3Attention / Eagle3DecoderLayer / Eagle3DraftModel / Eagle3ForCausalLM)
- KV + attention meta:
  - `tensorrt_llm/_torch/attention_backend/trtllm.py`
  - `tensorrt_llm/_torch/metadata.py` (`KVCacheParams`, `AttentionMetadata`)
- E2E & Eagle‑3 tests:
  - `tests/unittest/_torch/speculative/test_eagle3.py`
  - `tests/unittest/_torch/speculative/test_kv_cache_reuse.py`
- Docs:
  - `docs/source/blogs/tech_blog/blog11_GPT_OSS_Eagle3.md`
  - `docs/source/advanced/speculative/eagle3_*` (if present)

**Where to read (EAGLE repo side):**
- `EAGLE/eagle/modeling_eagle.py`
- EAGLE‑3 training/inference scripts:
  - `EAGLE/eagle/traineagle3/*`
  - `EAGLE/eagle/application/*`

**Deliverables (doc to live under `docs/` – name TBC):**

- **Design doc**: see `docs/turbomind_eagle3_trtllm_design.md` for the
  current reference implementation of 1.1–1.3.

- [x] **1.1 Eagle‑3 inference loop (TRT‑LLM) – GPT‑OSS‑120B**
  - Diagram the flow for *one step* of speculative decoding:
    - Base model forward (GPT‑OSS‑120B) with KV reuse.
    - Hidden capture for Eagle‑3 (which layers; how concatenated).
    - Eagle‑3 FC + attention + MLP + LM head → draft logits.
    - Tree structure construction (static tree for Eagle‑3).
    - Acceptance and KV cache updates.
  - Include:
    - `Eagle3ResourceManager.hidden_states` shape:
      `[max_tokens, hidden_size * num_capture_layers]`.
    - How `layers_to_capture` translates into the FC input layout.
    - How `max_draft_len` and `max_total_draft_tokens` feed into KV
      over‑provisioning (extra KV tokens).

- [x] **1.2 Tensor shapes at each Eagle‑3 draft stage**
  - For GPT‑OSS‑120B‑Eagle3 specifically:
    - Pre‑FC input: `[B, hidden * num_capture_layers]` or equivalent.
    - `fc.weight` from Eagle‑3 draft: `[hidden, hidden * N]` (EAGLE code +
      HF checkpoint; N≈3 for current GPT‑OSS checkpoint).
    - q/k/v/o projections:
      - HF/EAGLE: `midlayer.self_attn.{q,k,v,o}_proj.weight`.
      - TRT‑LLM: shapes used in attention kernels.
    - Attention outputs and LM head input/output:
      - context vector dimension,
      - LM head weight layout (row/column major).

- [x] **1.3 Mapping TRT‑LLM ↔ TurboMind components**
  - TRT‑LLM:
    - Eagle‑3 PyTorch stack:
      - `Eagle3SpecMetadata`, `Eagle3ResourceManager`,
      - Eagle‑3 **one‑model** worker / sampler,
      - drafting loops & KV reuse.
    - Engine pieces:
      - draft QKV + attention kernels,
      - KV layout (paged KV, extra KV tokens per sequence).
  - TurboMind:
    - `EagleModule` / `Eagle3DraftLayerWeight` / `Eagle3DraftLayer`.
    - `UnifiedDecoder::ForwardDraft`.
    - `LlamaV2::dynamicDecodeWithSpecMulti` /
      `runEagle3DraftTreeDecode` / `runEagleTargetTreeDecode`.
    - `SequenceManager`, KV layout, block manager.
  - The doc should state *explicitly* which TRT‑LLM behaviours we are
    reproducing and which we intentionally approximate (e.g. if we keep
    single‑position draft attention initially).

---

## 2. Phase 1 – Converter & Weight Layout (EAGLE‑3 Draft)  – **Owner: B**

Current state:
- **[x]** `lmdeploy/turbomind/eagle_draft_converter.py`:
  - Detects `eagle3_midlayer` layout by inspecting HF keys.
  - Writes `config.yaml` with:
    - `hidden_units`, `vocab_size`, `head_num`, `size_per_head`,
      `inter_size`,
    - Eagle‑3 metadata: `eagle_q_size`, `eagle_kv_size`,
      `eagle_qkv_in_dim`, `eagle_fc_in_dim`, `eagle_mode="eagle3"`.
  - `_convert_eagle3_midlayer`:
    - Writes norms, MLP weights from `midlayer.*`.
    - Validates `fc.weight` shape `(hidden, 3 * hidden)`.
    - Exports:
      - `fc.weight` (legacy shallow path),
      - `eagle_fc.weight` (`[3*hidden, hidden]`).
    - Builds real attention weights **from the base model’s** layer‑0
      Q/K/V/O into TurboMind’s LlamaAttention layout:
      - `layers.0.attention.w_qkv.weight`: `[hidden, 3 * hidden]`.
      - `layers.0.attention.wo.weight`:  `[attn_hidden, hidden]` (draft).

**TODOs:**

- [ ] **2.1 Lock in Eagle‑3 geometry for GPT‑OSS‑120B**
  - Using HF configs:
    - `openai/gpt-oss-120b`,
    - `nvidia/gpt-oss-120b-Eagle3`,
    - Eagle repo’s `EAGLEConfig` in `modeling_eagle.py`.
  - Document in the design doc:
    - `hidden_size`, `intermediate_size`,
    - `num_attention_heads`, `num_key_value_heads`, head dim,
    - Eagle‑3 FC fan‑in factor (`eagle_fc_in_factor`) and
      Q/K/V sizes used in the draft head.

- [x] **2.2 Reconcile converter layout with TRT‑LLM Eagle‑3**
  - Current converter behaviour (`lmdeploy/turbomind/eagle_draft_converter.py`):
    - detects `eagle3_midlayer` layout and writes:
      - `eagle_fc.weight` (`[3*hidden, hidden]`) from `fc.weight`,
      - native Eagle‑3 midlayer projections to:
        - `eagle3.q_proj.weight`,
        - `eagle3.k_proj.weight`,
        - `eagle3.v_proj.weight`,
        - `eagle3.o_proj.weight`,
        enforcing strict shape checks against the geometry inferred by
        `_infer_eagle3_geometry`.
    - when `base_model_dir` is provided, additionally exports a
      LLaMA‑layout attention backbone from the target model’s
      `model.layers.0.self_attn.{q,k,v,o}_proj.weight` into:
      - `layers.0.attention.w_qkv.weight`,
      - `layers.0.attention.wo.weight`,
      for use by the shallow draft path.
    - records Eagle‑3 geometry (`eagle_q_size`, `eagle_kv_size`,
      `eagle_qkv_in_dim`, `eagle_fc_in_dim`, and factors) in
      `config.yaml`, which `EagleModule::load` consumes to allocate
      `EagleWeight` and initialise `Eagle3AttentionWeight`.

- [ ] **2.3 Strengthen config + shape validation**
  - Add explicit unit tests under `tests/turbomind` to:
    - run `prepare_eagle_draft_from_hf` on a small Eagle‑3 checkpoint
      (or a synthetic subset),
    - read back `config.yaml` and all `.weight` files,
    - assert shapes/dtypes:
      - `layers.0.attention.w_qkv.weight`,
      - `layers.0.attention.wo.weight`,
      - `fc.weight`, `eagle_fc.weight`,
      - MLP and norm weights.
  - Converter should:
    - **fail fast** with clear messages if the HF checkpoint deviates
      from the expected Eagle‑3 layout (rather than silently producing
      an approximate draft).

---

## 3. Phase 2 – Eagle‑3 Draft Attention & Capture  – **Owner: A**

Current state:
- **[x]** `Eagle3DraftLayerWeight` aggregates:
  - `LlamaAttentionWeight` + `LlamaFfnWeight` + norm tensors.
- **[x]** `Eagle3DraftLayer::Forward`:
  - `input_norm` RMSNorm → attention backend → `post_attn_norm` RMSNorm →
    FFN → residual + `output_norm` RMSNorm.
  - Today the attention backend is:
    - `UnifiedAttentionLayer` when geometry matches standard Llama
      (Q/K/V/O all `hidden × hidden`, fused QKV `[hidden, 3*hidden]`),
    - **fallback** shallow QKV+V→Wo path otherwise.
  - This is **not compatible** with GPT‑OSS‑120B‑Eagle3 midlayer shapes:
    - `fc.weight`         = `[hidden=2880, 3*hidden=8640]`
    - `q_proj.weight`     = `[4096, 2880]`
    - `k_proj/v_proj`     = `[512, 2880]`
    - `o_proj.weight`     = `[2880, 4096]`
    - i.e. non‑square, non‑uniform Q/K/V dims that cannot be “reshaped”
      into a standard Llama attention layout without changing the math.
  - Robust shape/dtype checks are already in place and correctly refuse
    to “magically” reinterpret these weights.
- **[x]** `UnifiedDecoder::ForwardDraft` calls `Eagle3DraftLayer` when
          present; draft logits are produced via `LlamaLinear` LM head.

Known gap:
- **[ ]** There is **no dedicated Eagle‑3 attention backend** today:
  - GPT‑OSS‑120B‑Eagle3 midlayer geometry cannot be expressed via
    `UnifiedAttentionLayer` without rewriting kernels.
  - Any attempt to reshape `[4096×2880, 512×2880, 2880×4096]` weights
    into a `[hidden, 3*hidden]` Llama layout would produce numerically
    wrong and unstable behaviour, and would not match TRT‑LLM.
  - Draft attention also currently runs with **q_len = kv_len = 1**
    (single‑position semantics) and does **not** reuse KV along the
    tree like TRT‑LLM Eagle‑3.

**TODOs:**

- [ ] **3.1 Introduce a dedicated Eagle‑3 attention backend**
  - Add an `Eagle3AttentionWeight` struct that holds:
    - Q: `[q_dim, hidden]`   (e.g. 4096×2880),
    - K/V: `[kv_dim, hidden]` (e.g. 512×2880),
    - O: `[hidden, q_dim]`   (e.g. 2880×4096),
    - any head metadata needed to map these into heads for CUDA kernels.
  - Implement an `Eagle3AttentionLayer` with its own CUDA kernels that:
    - consumes FC‑projected Eagle‑3 input (e.g. `[B, hidden]` or `[B, L, hidden]`),
    - applies Q/K/V projections with the Eagle‑3 geometry,
    - applies RoPE and softmax exactly as in EAGLE3/TRT‑LLM,
    - produces context vectors in the expected dimension (e.g. q_dim),
      followed by `o_proj` back into `hidden`.
  - Plug `Eagle3AttentionLayer` into `Eagle3DraftLayer::Forward` when
    `eagle_mode == "eagle3"` and the weight shapes match GPT‑OSS‑120B,
    instead of going through `UnifiedAttentionLayer`.
  - Keep strict shape checks; on mismatch:
    - log a clear `[EAGLE3][Attention][fallback]` warning,
    - fall back to a pass‑through draft (no speculative gains), but do
      **not** attempt to reinterpret weights.

- [ ] **3.2 Guarantee real Eagle‑3 attention in normal Eagle‑3 runs**
  - For GPT‑OSS‑120B‑Eagle3 (and other supported Eagle‑3 drafts):
    - ensure `EagleModule::load` builds `Eagle3AttentionWeight` from
      `midlayer.self_attn.{q,k,v,o}_proj.weight` and exposes it to
      `Eagle3DraftLayer`,
    - ensure `Eagle3DraftLayer::Forward` always uses
      `Eagle3AttentionLayer` (and never the shallow QKV path) under
      valid configs.
  - Add a debug counter / log (gated by `eagle_debug`) that reports,
    per run:
    - how often Eagle‑3 attention is used,
    - how often we fall back to pass‑through or legacy paths.

- [ ] **3.3 Align draft FC + capture layout with EAGLE/TRT‑LLM**
  - From EAGLE + TRT‑LLM:
    - Confirm which layers’ hidden states are concatenated into the
      Eagle‑3 FC input (e.g. `[low, mid, high]` layer selection).
    - Confirm the FC input ordering and any norms applied before FC.
  - In TurboMind:
    - Verify that `eagle_capture_hidden_` and `eagle_fc_in_dim_`
      correspond to the same capture pattern.
    - If necessary, adjust the capture ordering / concatenation in
      `UnifiedDecoder` and `EagleModule::load` so FC sees the same
      feature arrangement as the Eagle‑3 training setup.

- [ ] **3.4 (Optional for strict TRT parity) multi‑position draft attention**
  - If TRT‑LLM uses **q_len>1** in the draft tree for Eagle‑3:
    - Add a `ForwardTree` or extended `Forward` that:
      - accepts `[B, q_len, hidden]`,
      - reuses KV for draft tokens exactly as the main decoder does.
    - Wire this from `runEagle3DraftTreeDecode` so draft tokens per
      layer are produced using the same attention geometry as TRT‑LLM.
  - If TRT‑LLM keeps q_len=1 for Eagle‑3, document that and mark this
    item as “not required”.

- [ ] **3.5 Clean up shallow draft path once numerics are good**
  - Once 4.x validation (below) shows Eagle‑3 numerics are solid:
    - Restrict the shallow QKV path to:
      - explicit debug builds, or
      - non‑Eagle‑3 legacy modes.
    - Ensure `EagleModule` does **not** participate in Eagle‑3 drafting
      at all (only UnifiedDecoder + Eagle‑3 draft layer).

---

## 4. Phase 3 – TurboMind Integration & KV Semantics  – **Owner: C**

Current state (TurboMind side):
- **[x]** `LlamaV2::dynamicDecodeWithSpecMulti`:
  - For `spec_method == "eagle3"`:
    - calls `runEagle3DraftTreeDecode` to generate draft tokens +
      initial target tokens,
    - builds a tree via `eagle::SpeculationTree` using default choices
      from `EagleModule::getDefaultChoices()`,
    - mirrors paths into `EagleBuffers::inputs.draft_paths`,
    - builds leaf + packed masks (tree attention + acceptance),
    - uses device‑side acceptance (`invokeTreeAcceptByIdsWithPaths`,
      `invokePackAcceptedPaths`),
    - commits extra tokens via `DynamicDecodeLayer::ForwardMultiStep`
      + KV rewind driven by committed tails.
- **[x]** `targetTreeDecode` + `runEagleTargetTreeDecode`:
  - prepare tree inputs (`eagle_net_*` tensors),
  - compact tree tokens, embed them, and run a **scratch‑KV** target
    decode pass that reuses prefix KV read‑only,
  - write per‑node `target_tokens` back into `EagleBuffers`.
- **[x]** KV invariants:
  - Tree decode never mutates live `SequenceManager` KV:
    - prefix blocks reused as read‑only,
    - tree tokens live in scratch KV and are discarded,
    - only `KVCacheRewind` mutates KV after acceptance.
- **[x]** SpecPV path:
  - partial KV cache (`PartialKVCache`) can be used instead of full
    prefix KV for target‑tree decode, with clear fallbacks on mismatch.

**TODOs:**

- [ ] **4.1 Formalise target‑tree + acceptance contract vs TRT‑LLM**
  - In the design doc, explicitly compare:
    - TurboMind’s `eagle::SpeculationTree` default choices +
      packed masks,
    - TRT‑LLM’s Eagle‑3 tree semantics:
      - static vs dynamic tree,
      - branching factor and depth for GPT‑OSS‑120B,
      - acceptance algorithm (ID equality, root token behaviour).
  - Confirm that:
    - TurboMind’s `invokeTreeAcceptByIdsWithPaths` and
      `invokePackAcceptedPaths` implement the same semantics as
      TRT‑LLM for the static tree used in GPT‑OSS‑120B‑Eagle3.

- [ ] **4.2 Dynamic batching, block reuse, and KV invariants**
  - Re‑run and extend KV‑related tests with EAGLE‑3 enabled:
    - existing TurboMind KV tests,
    - TRT‑LLM tests like `test_kv_lens_runtime_with_eagle3_one_model`
      as inspiration (num_extra_kv_tokens semantics).
  - For TurboMind:
    - assert that `Sequence::blocks` and `cache_len` are consistent
      after many speculative steps with variable batch sizes.
    - ensure KV over‑provisioning for EAGLE matches what TRT‑LLM does
      for `max_draft_len` / `max_total_draft_tokens` (conceptually).

- [ ] **4.3 LMDeploy integration surface**
  - Ensure LMDeploy users can configure Eagle‑3 in a way that mirrors
    TRT‑LLM’s `EagleDecodingConfig`:
    - `method="eagle3"`, `num_speculative_tokens`, `max_draft_len`,
      `eagle_choices` (optional override), `use_dynamic_tree` (if
      supported), `max_total_draft_tokens`.
  - Validate that:
    - offline `pipeline(...)` and `TurbomindEngineConfig` accept
      Eagle‑3 config cleanly,
    - misconfigurations fail fast with clear messages.

---

## 5. Phase 4 – Numeric & Behavioural Validation  – **Owner: D**

Goal: prove TurboMind+EAGLE‑3 behaves like TRT‑LLM Eagle‑3 on
GPT‑OSS‑120B, within agreed tolerances.

**5.1 Stage‑wise draft compare (HF/TRT vs TurboMind)**
- [ ] Extend `tests/turbomind/eagle3_compare.py` to:
  - call a debug binding (`_turbomind.eagle3_forward_debug`) that:
    - runs `Eagle3DraftLayer::Forward` + LM head,
    - exposes intermediate tensors:
      - `fc_out`, `attn_out`, `ffn_out`, `pre_head_hidden`, `logits`.
  - For each stage available from HF/TRT and TurboMind:
    - compute `mean_abs_diff`, `max_abs_diff`, cosine similarity.
  - For `logits` specifically:
    - compute argmax match rate and top‑k overlap (e.g. top‑5).
- [ ] Define tolerances (documented in the design doc), e.g.:
  - cosine(logits) > 0.98,
  - top‑5 overlap > 0.8,
  - max_abs_diff within expected BF16 / MXFP ranges.

**5.2 Speculative acceptance metrics vs TRT‑LLM**
- [ ] Use `SpeculativeDecodingStats` / `EagleMetricsSummary` and
      TRT‑LLM’s test patterns to compare:
  - `mean_acceptance_length`,
  - `fraction_steps_accept_ge2`,
  - `eagle_total_draft_tokens`, `eagle_total_accepted_tokens`.
- [ ] On GPT‑OSS‑120B‑Eagle3:
  - run the same prompt sets in:
    - TRT‑LLM (per blog11),
    - TurboMind + LMDeploy,
  - compare acceptance distributions; investigate discrepancies beyond
    an agreed threshold.

**5.3 Throughput benchmarks**
- [ ] Extend `inference/benchmark_speculative.py` / `run_spec_suite.sh`
      with a GPT‑OSS‑120B‑Eagle3 profile:
  - baseline (no spec),
  - Eagle‑3 (TurboMind),
  - optionally TRT‑LLM for reference.
- [ ] Target:
  - TurboMind Eagle‑3 ≥ TurboMind baseline throughput,
  - TurboMind Eagle‑3 ≈ TRT‑LLM Eagle‑3 on the same hardware class in
    key scenarios (8K/32K contexts, realistic prompts).

**5.4 Regression & CI**
- [ ] Add a minimal CI spec test:
  - small Eagle‑3 draft (or synthetic),
  - one short prompt,
  - asserts:
    - no EAGLE fallback warnings,
    - acceptance is neither 0 nor “always full accept”,
    - token stream is stable against a recorded golden log
      (within allowed stochasticity).

---

## 6. Phase 5 – SpecPV (Partial KV) Follow‑Ups  – **Owner: A/C**

Current state:
- **[x]** `SpecPVCacheConfig` + `PartialKVCache`:
  - segments (sink, retrieval, window, buffer),
  - Kmax/Kmin summaries, retrieval scoring, update/refresh/reset.
- **[x]** `initSpecPVFromFullKV` flattens full KV prefix and seeds partial
          KV cache.
- **[x]** `runEagleTargetTreeDecode` can run tree decode against partial KV:
  - builds scratch KV blocks from `PartialKVCache::active_prefix`,
  - uses scratch KV for prefix + tree; live KV untouched.
- **[x]** `updateSpecPVAfterAcceptance`:
  - reads committed tail lengths from `ForwardMultiStep`,
  - calls `PartialKVCache::update_after_acceptance` per layer/slot,
  - robustly disables SpecPV on any invariant failure.

**TODOs (after Eagle‑3 core is solid):**

- [ ] **6.1 Validate semantics with EAGLE‑3**
  - For long‑context runs:
    - verify token streams with SpecPV enabled vs disabled are
      identical (within numerical noise),
    - confirm EAGLE metrics (acceptance lengths, EOS handling) are
      unchanged by SpecPV.

- [ ] **6.2 Tune SpecPV geometry for GPT‑OSS‑120B‑Eagle3**
  - Choose default `specpv_*` parameters (block sizes, retrieval window,
    buffer) that:
    - give a measurable reduction in KV tokens attended during tree
      decode at long context,
    - maintain acceptance metrics close to full‑KV runs.

- [ ] **6.3 Perf benchmarks with SpecPV**
  - Extend the speculative benchmark suite with SpecPV‑enabled runs:
    - throughput vs full‑KV Eagle‑3 at 32K/64K context,
    - memory usage and KV bandwidth comparisons.

When all the above phases are checked off, TurboMind’s EAGLE‑3 path
should be functionally, numerically, and performance‑wise aligned
with TensorRT‑LLM Eagle‑3 on GPT‑OSS‑120B, with SpecPV as an optional
long‑context optimisation layer on top.
